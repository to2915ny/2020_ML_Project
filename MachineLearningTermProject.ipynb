{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineLearningTermProject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea7n4l1BEjQY"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from torch import nn, from_numpy, optim\r\n",
        "import numpy as np"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hECq4aYbEn4a"
      },
      "source": [
        "class customDataset(Dataset):\r\n",
        "\r\n",
        "  def __init__(self,filepath):\r\n",
        "    xy = np.loadtxt(filepath, delimiter=',',dtype=np.float32)\r\n",
        "    self.len= xy.shape[0]\r\n",
        "    self.x_data = from_numpy(xy[:,1:])\r\n",
        "    self.x_data = np.where(self.x_data==-1,2,self.x_data) #change -1 to 2\r\n",
        "    self.y_data = from_numpy(xy[:,[0]]).flatten().long()\r\n",
        "    self.y_data = np.where(self.y_data==-1,0,self.y_data) # change -1 to 0\r\n",
        "\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "    return self.x_data[index],self.y_data[index]\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.len"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NHqH19EHga9"
      },
      "source": [
        "Trdataset = customDataset('dota2Train.csv')\r\n",
        "Tsdataset = customDataset('dota2Test.csv')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLYipUeZHwXm",
        "outputId": "40f30b8f-b6d3-492b-bf1d-af904d9cf402"
      },
      "source": [
        "print(Trdataset.y_data.size)\r\n",
        "\r\n",
        "train_loader = DataLoader(dataset=Trdataset,\r\n",
        "                          batch_size=64,\r\n",
        "                          shuffle=True,\r\n",
        "                          )\r\n",
        "test_loader = DataLoader(dataset=Tsdataset,\r\n",
        "                          batch_size=64,\r\n",
        "                          shuffle=True,\r\n",
        "                          )\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "92650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g84a7pt4IDFl"
      },
      "source": [
        "class Model(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "\r\n",
        "    super(Model,self).__init__()\r\n",
        "    self.l1 = nn.Linear(116,256)\r\n",
        "    self.l2 = nn.Linear(256,128)\r\n",
        "    self.l3 = nn.Linear(128,64)\r\n",
        "    self.l4 = nn.Linear(64,32)\r\n",
        "    self.l5 = nn.Linear(32,2)\r\n",
        "\r\n",
        "    self.hidden = nn.Sequential(\r\n",
        "        self.l1,\r\n",
        "        nn.ReLU(),\r\n",
        "        self.l2,\r\n",
        "        nn.ReLU(),\r\n",
        "        self.l3,\r\n",
        "        nn.ReLU(),\r\n",
        "        self.l4,\r\n",
        "        nn.ReLU(),\r\n",
        "        self.l5\r\n",
        "       \r\n",
        "            )\r\n",
        "  \r\n",
        "\r\n",
        "  def forward(self,x):\r\n",
        "    x = self.hidden(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "  "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umMkrGIeK5UD",
        "outputId": "488d73c6-97e4-4f89-a0e9-791e7d53c66d"
      },
      "source": [
        "model = Model()\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()  \r\n",
        "#criterion = nn.BCEWithLogitsLoss()\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\r\n",
        "\r\n",
        "for epoch in range(5):\r\n",
        "  for i, data in enumerate(train_loader,0):\r\n",
        "    inputs, labels = data\r\n",
        "    y_pred = model(inputs)\r\n",
        "    \r\n",
        "    loss = criterion(y_pred,labels)\r\n",
        "    \r\n",
        "    if i % 10 ==0:\r\n",
        "      print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\r\n",
        "                epoch, i * len(data), len(train_loader.dataset),\r\n",
        "                100. * i / len(train_loader), loss.item()))\r\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 | Batch Status: 0/92650 (0%) | Loss: 0.687192\n",
            "Train Epoch: 0 | Batch Status: 20/92650 (1%) | Loss: 0.713664\n",
            "Train Epoch: 0 | Batch Status: 40/92650 (1%) | Loss: 0.699623\n",
            "Train Epoch: 0 | Batch Status: 60/92650 (2%) | Loss: 0.690019\n",
            "Train Epoch: 0 | Batch Status: 80/92650 (3%) | Loss: 0.679102\n",
            "Train Epoch: 0 | Batch Status: 100/92650 (3%) | Loss: 0.677589\n",
            "Train Epoch: 0 | Batch Status: 120/92650 (4%) | Loss: 0.689609\n",
            "Train Epoch: 0 | Batch Status: 140/92650 (5%) | Loss: 0.694548\n",
            "Train Epoch: 0 | Batch Status: 160/92650 (6%) | Loss: 0.695631\n",
            "Train Epoch: 0 | Batch Status: 180/92650 (6%) | Loss: 0.712864\n",
            "Train Epoch: 0 | Batch Status: 200/92650 (7%) | Loss: 0.692304\n",
            "Train Epoch: 0 | Batch Status: 220/92650 (8%) | Loss: 0.694447\n",
            "Train Epoch: 0 | Batch Status: 240/92650 (8%) | Loss: 0.692748\n",
            "Train Epoch: 0 | Batch Status: 260/92650 (9%) | Loss: 0.690040\n",
            "Train Epoch: 0 | Batch Status: 280/92650 (10%) | Loss: 0.692927\n",
            "Train Epoch: 0 | Batch Status: 300/92650 (10%) | Loss: 0.692938\n",
            "Train Epoch: 0 | Batch Status: 320/92650 (11%) | Loss: 0.693415\n",
            "Train Epoch: 0 | Batch Status: 340/92650 (12%) | Loss: 0.680636\n",
            "Train Epoch: 0 | Batch Status: 360/92650 (12%) | Loss: 0.688753\n",
            "Train Epoch: 0 | Batch Status: 380/92650 (13%) | Loss: 0.703687\n",
            "Train Epoch: 0 | Batch Status: 400/92650 (14%) | Loss: 0.692852\n",
            "Train Epoch: 0 | Batch Status: 420/92650 (15%) | Loss: 0.690795\n",
            "Train Epoch: 0 | Batch Status: 440/92650 (15%) | Loss: 0.686480\n",
            "Train Epoch: 0 | Batch Status: 460/92650 (16%) | Loss: 0.695172\n",
            "Train Epoch: 0 | Batch Status: 480/92650 (17%) | Loss: 0.689213\n",
            "Train Epoch: 0 | Batch Status: 500/92650 (17%) | Loss: 0.694841\n",
            "Train Epoch: 0 | Batch Status: 520/92650 (18%) | Loss: 0.696164\n",
            "Train Epoch: 0 | Batch Status: 540/92650 (19%) | Loss: 0.694865\n",
            "Train Epoch: 0 | Batch Status: 560/92650 (19%) | Loss: 0.691829\n",
            "Train Epoch: 0 | Batch Status: 580/92650 (20%) | Loss: 0.690960\n",
            "Train Epoch: 0 | Batch Status: 600/92650 (21%) | Loss: 0.696172\n",
            "Train Epoch: 0 | Batch Status: 620/92650 (21%) | Loss: 0.689434\n",
            "Train Epoch: 0 | Batch Status: 640/92650 (22%) | Loss: 0.686921\n",
            "Train Epoch: 0 | Batch Status: 660/92650 (23%) | Loss: 0.693158\n",
            "Train Epoch: 0 | Batch Status: 680/92650 (23%) | Loss: 0.707462\n",
            "Train Epoch: 0 | Batch Status: 700/92650 (24%) | Loss: 0.691283\n",
            "Train Epoch: 0 | Batch Status: 720/92650 (25%) | Loss: 0.691209\n",
            "Train Epoch: 0 | Batch Status: 740/92650 (26%) | Loss: 0.691195\n",
            "Train Epoch: 0 | Batch Status: 760/92650 (26%) | Loss: 0.689922\n",
            "Train Epoch: 0 | Batch Status: 780/92650 (27%) | Loss: 0.690539\n",
            "Train Epoch: 0 | Batch Status: 800/92650 (28%) | Loss: 0.697751\n",
            "Train Epoch: 0 | Batch Status: 820/92650 (28%) | Loss: 0.688562\n",
            "Train Epoch: 0 | Batch Status: 840/92650 (29%) | Loss: 0.694190\n",
            "Train Epoch: 0 | Batch Status: 860/92650 (30%) | Loss: 0.688827\n",
            "Train Epoch: 0 | Batch Status: 880/92650 (30%) | Loss: 0.681645\n",
            "Train Epoch: 0 | Batch Status: 900/92650 (31%) | Loss: 0.693011\n",
            "Train Epoch: 0 | Batch Status: 920/92650 (32%) | Loss: 0.703614\n",
            "Train Epoch: 0 | Batch Status: 940/92650 (32%) | Loss: 0.685242\n",
            "Train Epoch: 0 | Batch Status: 960/92650 (33%) | Loss: 0.695170\n",
            "Train Epoch: 0 | Batch Status: 980/92650 (34%) | Loss: 0.693602\n",
            "Train Epoch: 0 | Batch Status: 1000/92650 (35%) | Loss: 0.684050\n",
            "Train Epoch: 0 | Batch Status: 1020/92650 (35%) | Loss: 0.699933\n",
            "Train Epoch: 0 | Batch Status: 1040/92650 (36%) | Loss: 0.693823\n",
            "Train Epoch: 0 | Batch Status: 1060/92650 (37%) | Loss: 0.692203\n",
            "Train Epoch: 0 | Batch Status: 1080/92650 (37%) | Loss: 0.687117\n",
            "Train Epoch: 0 | Batch Status: 1100/92650 (38%) | Loss: 0.692669\n",
            "Train Epoch: 0 | Batch Status: 1120/92650 (39%) | Loss: 0.693202\n",
            "Train Epoch: 0 | Batch Status: 1140/92650 (39%) | Loss: 0.691074\n",
            "Train Epoch: 0 | Batch Status: 1160/92650 (40%) | Loss: 0.695493\n",
            "Train Epoch: 0 | Batch Status: 1180/92650 (41%) | Loss: 0.689181\n",
            "Train Epoch: 0 | Batch Status: 1200/92650 (41%) | Loss: 0.688411\n",
            "Train Epoch: 0 | Batch Status: 1220/92650 (42%) | Loss: 0.688923\n",
            "Train Epoch: 0 | Batch Status: 1240/92650 (43%) | Loss: 0.697927\n",
            "Train Epoch: 0 | Batch Status: 1260/92650 (44%) | Loss: 0.685865\n",
            "Train Epoch: 0 | Batch Status: 1280/92650 (44%) | Loss: 0.690048\n",
            "Train Epoch: 0 | Batch Status: 1300/92650 (45%) | Loss: 0.695465\n",
            "Train Epoch: 0 | Batch Status: 1320/92650 (46%) | Loss: 0.687790\n",
            "Train Epoch: 0 | Batch Status: 1340/92650 (46%) | Loss: 0.693353\n",
            "Train Epoch: 0 | Batch Status: 1360/92650 (47%) | Loss: 0.696961\n",
            "Train Epoch: 0 | Batch Status: 1380/92650 (48%) | Loss: 0.688790\n",
            "Train Epoch: 0 | Batch Status: 1400/92650 (48%) | Loss: 0.688039\n",
            "Train Epoch: 0 | Batch Status: 1420/92650 (49%) | Loss: 0.699712\n",
            "Train Epoch: 0 | Batch Status: 1440/92650 (50%) | Loss: 0.691293\n",
            "Train Epoch: 0 | Batch Status: 1460/92650 (50%) | Loss: 0.696116\n",
            "Train Epoch: 0 | Batch Status: 1480/92650 (51%) | Loss: 0.684969\n",
            "Train Epoch: 0 | Batch Status: 1500/92650 (52%) | Loss: 0.687772\n",
            "Train Epoch: 0 | Batch Status: 1520/92650 (52%) | Loss: 0.690220\n",
            "Train Epoch: 0 | Batch Status: 1540/92650 (53%) | Loss: 0.704015\n",
            "Train Epoch: 0 | Batch Status: 1560/92650 (54%) | Loss: 0.692669\n",
            "Train Epoch: 0 | Batch Status: 1580/92650 (55%) | Loss: 0.692706\n",
            "Train Epoch: 0 | Batch Status: 1600/92650 (55%) | Loss: 0.687681\n",
            "Train Epoch: 0 | Batch Status: 1620/92650 (56%) | Loss: 0.714383\n",
            "Train Epoch: 0 | Batch Status: 1640/92650 (57%) | Loss: 0.693498\n",
            "Train Epoch: 0 | Batch Status: 1660/92650 (57%) | Loss: 0.684579\n",
            "Train Epoch: 0 | Batch Status: 1680/92650 (58%) | Loss: 0.681119\n",
            "Train Epoch: 0 | Batch Status: 1700/92650 (59%) | Loss: 0.678669\n",
            "Train Epoch: 0 | Batch Status: 1720/92650 (59%) | Loss: 0.695323\n",
            "Train Epoch: 0 | Batch Status: 1740/92650 (60%) | Loss: 0.693406\n",
            "Train Epoch: 0 | Batch Status: 1760/92650 (61%) | Loss: 0.681930\n",
            "Train Epoch: 0 | Batch Status: 1780/92650 (61%) | Loss: 0.701261\n",
            "Train Epoch: 0 | Batch Status: 1800/92650 (62%) | Loss: 0.695143\n",
            "Train Epoch: 0 | Batch Status: 1820/92650 (63%) | Loss: 0.713428\n",
            "Train Epoch: 0 | Batch Status: 1840/92650 (64%) | Loss: 0.706648\n",
            "Train Epoch: 0 | Batch Status: 1860/92650 (64%) | Loss: 0.697217\n",
            "Train Epoch: 0 | Batch Status: 1880/92650 (65%) | Loss: 0.696799\n",
            "Train Epoch: 0 | Batch Status: 1900/92650 (66%) | Loss: 0.688511\n",
            "Train Epoch: 0 | Batch Status: 1920/92650 (66%) | Loss: 0.689649\n",
            "Train Epoch: 0 | Batch Status: 1940/92650 (67%) | Loss: 0.686403\n",
            "Train Epoch: 0 | Batch Status: 1960/92650 (68%) | Loss: 0.691807\n",
            "Train Epoch: 0 | Batch Status: 1980/92650 (68%) | Loss: 0.697227\n",
            "Train Epoch: 0 | Batch Status: 2000/92650 (69%) | Loss: 0.694258\n",
            "Train Epoch: 0 | Batch Status: 2020/92650 (70%) | Loss: 0.692781\n",
            "Train Epoch: 0 | Batch Status: 2040/92650 (70%) | Loss: 0.691848\n",
            "Train Epoch: 0 | Batch Status: 2060/92650 (71%) | Loss: 0.689416\n",
            "Train Epoch: 0 | Batch Status: 2080/92650 (72%) | Loss: 0.701825\n",
            "Train Epoch: 0 | Batch Status: 2100/92650 (73%) | Loss: 0.691195\n",
            "Train Epoch: 0 | Batch Status: 2120/92650 (73%) | Loss: 0.695173\n",
            "Train Epoch: 0 | Batch Status: 2140/92650 (74%) | Loss: 0.681292\n",
            "Train Epoch: 0 | Batch Status: 2160/92650 (75%) | Loss: 0.688004\n",
            "Train Epoch: 0 | Batch Status: 2180/92650 (75%) | Loss: 0.698020\n",
            "Train Epoch: 0 | Batch Status: 2200/92650 (76%) | Loss: 0.692898\n",
            "Train Epoch: 0 | Batch Status: 2220/92650 (77%) | Loss: 0.697561\n",
            "Train Epoch: 0 | Batch Status: 2240/92650 (77%) | Loss: 0.694024\n",
            "Train Epoch: 0 | Batch Status: 2260/92650 (78%) | Loss: 0.700675\n",
            "Train Epoch: 0 | Batch Status: 2280/92650 (79%) | Loss: 0.695573\n",
            "Train Epoch: 0 | Batch Status: 2300/92650 (79%) | Loss: 0.693598\n",
            "Train Epoch: 0 | Batch Status: 2320/92650 (80%) | Loss: 0.691872\n",
            "Train Epoch: 0 | Batch Status: 2340/92650 (81%) | Loss: 0.692684\n",
            "Train Epoch: 0 | Batch Status: 2360/92650 (81%) | Loss: 0.700851\n",
            "Train Epoch: 0 | Batch Status: 2380/92650 (82%) | Loss: 0.689246\n",
            "Train Epoch: 0 | Batch Status: 2400/92650 (83%) | Loss: 0.689503\n",
            "Train Epoch: 0 | Batch Status: 2420/92650 (84%) | Loss: 0.687148\n",
            "Train Epoch: 0 | Batch Status: 2440/92650 (84%) | Loss: 0.699474\n",
            "Train Epoch: 0 | Batch Status: 2460/92650 (85%) | Loss: 0.691231\n",
            "Train Epoch: 0 | Batch Status: 2480/92650 (86%) | Loss: 0.688861\n",
            "Train Epoch: 0 | Batch Status: 2500/92650 (86%) | Loss: 0.695585\n",
            "Train Epoch: 0 | Batch Status: 2520/92650 (87%) | Loss: 0.696142\n",
            "Train Epoch: 0 | Batch Status: 2540/92650 (88%) | Loss: 0.694225\n",
            "Train Epoch: 0 | Batch Status: 2560/92650 (88%) | Loss: 0.692771\n",
            "Train Epoch: 0 | Batch Status: 2580/92650 (89%) | Loss: 0.692780\n",
            "Train Epoch: 0 | Batch Status: 2600/92650 (90%) | Loss: 0.689140\n",
            "Train Epoch: 0 | Batch Status: 2620/92650 (90%) | Loss: 0.691757\n",
            "Train Epoch: 0 | Batch Status: 2640/92650 (91%) | Loss: 0.686279\n",
            "Train Epoch: 0 | Batch Status: 2660/92650 (92%) | Loss: 0.695040\n",
            "Train Epoch: 0 | Batch Status: 2680/92650 (93%) | Loss: 0.689753\n",
            "Train Epoch: 0 | Batch Status: 2700/92650 (93%) | Loss: 0.705388\n",
            "Train Epoch: 0 | Batch Status: 2720/92650 (94%) | Loss: 0.691227\n",
            "Train Epoch: 0 | Batch Status: 2740/92650 (95%) | Loss: 0.706705\n",
            "Train Epoch: 0 | Batch Status: 2760/92650 (95%) | Loss: 0.689742\n",
            "Train Epoch: 0 | Batch Status: 2780/92650 (96%) | Loss: 0.696020\n",
            "Train Epoch: 0 | Batch Status: 2800/92650 (97%) | Loss: 0.691621\n",
            "Train Epoch: 0 | Batch Status: 2820/92650 (97%) | Loss: 0.698130\n",
            "Train Epoch: 0 | Batch Status: 2840/92650 (98%) | Loss: 0.696665\n",
            "Train Epoch: 0 | Batch Status: 2860/92650 (99%) | Loss: 0.686283\n",
            "Train Epoch: 0 | Batch Status: 2880/92650 (99%) | Loss: 0.687847\n",
            "Train Epoch: 1 | Batch Status: 0/92650 (0%) | Loss: 0.698992\n",
            "Train Epoch: 1 | Batch Status: 20/92650 (1%) | Loss: 0.697569\n",
            "Train Epoch: 1 | Batch Status: 40/92650 (1%) | Loss: 0.687889\n",
            "Train Epoch: 1 | Batch Status: 60/92650 (2%) | Loss: 0.693456\n",
            "Train Epoch: 1 | Batch Status: 80/92650 (3%) | Loss: 0.696103\n",
            "Train Epoch: 1 | Batch Status: 100/92650 (3%) | Loss: 0.688857\n",
            "Train Epoch: 1 | Batch Status: 120/92650 (4%) | Loss: 0.702429\n",
            "Train Epoch: 1 | Batch Status: 140/92650 (5%) | Loss: 0.695758\n",
            "Train Epoch: 1 | Batch Status: 160/92650 (6%) | Loss: 0.693694\n",
            "Train Epoch: 1 | Batch Status: 180/92650 (6%) | Loss: 0.683091\n",
            "Train Epoch: 1 | Batch Status: 200/92650 (7%) | Loss: 0.688953\n",
            "Train Epoch: 1 | Batch Status: 220/92650 (8%) | Loss: 0.702651\n",
            "Train Epoch: 1 | Batch Status: 240/92650 (8%) | Loss: 0.701523\n",
            "Train Epoch: 1 | Batch Status: 260/92650 (9%) | Loss: 0.692676\n",
            "Train Epoch: 1 | Batch Status: 280/92650 (10%) | Loss: 0.692669\n",
            "Train Epoch: 1 | Batch Status: 300/92650 (10%) | Loss: 0.684987\n",
            "Train Epoch: 1 | Batch Status: 320/92650 (11%) | Loss: 0.696344\n",
            "Train Epoch: 1 | Batch Status: 340/92650 (12%) | Loss: 0.691904\n",
            "Train Epoch: 1 | Batch Status: 360/92650 (12%) | Loss: 0.693580\n",
            "Train Epoch: 1 | Batch Status: 380/92650 (13%) | Loss: 0.688818\n",
            "Train Epoch: 1 | Batch Status: 400/92650 (14%) | Loss: 0.693833\n",
            "Train Epoch: 1 | Batch Status: 420/92650 (15%) | Loss: 0.700061\n",
            "Train Epoch: 1 | Batch Status: 440/92650 (15%) | Loss: 0.699173\n",
            "Train Epoch: 1 | Batch Status: 460/92650 (16%) | Loss: 0.706609\n",
            "Train Epoch: 1 | Batch Status: 480/92650 (17%) | Loss: 0.690544\n",
            "Train Epoch: 1 | Batch Status: 500/92650 (17%) | Loss: 0.694263\n",
            "Train Epoch: 1 | Batch Status: 520/92650 (18%) | Loss: 0.689580\n",
            "Train Epoch: 1 | Batch Status: 540/92650 (19%) | Loss: 0.690889\n",
            "Train Epoch: 1 | Batch Status: 560/92650 (19%) | Loss: 0.692666\n",
            "Train Epoch: 1 | Batch Status: 580/92650 (20%) | Loss: 0.691178\n",
            "Train Epoch: 1 | Batch Status: 600/92650 (21%) | Loss: 0.705315\n",
            "Train Epoch: 1 | Batch Status: 620/92650 (21%) | Loss: 0.687180\n",
            "Train Epoch: 1 | Batch Status: 640/92650 (22%) | Loss: 0.691245\n",
            "Train Epoch: 1 | Batch Status: 660/92650 (23%) | Loss: 0.691506\n",
            "Train Epoch: 1 | Batch Status: 680/92650 (23%) | Loss: 0.692693\n",
            "Train Epoch: 1 | Batch Status: 700/92650 (24%) | Loss: 0.694853\n",
            "Train Epoch: 1 | Batch Status: 720/92650 (25%) | Loss: 0.687907\n",
            "Train Epoch: 1 | Batch Status: 740/92650 (26%) | Loss: 0.690030\n",
            "Train Epoch: 1 | Batch Status: 760/92650 (26%) | Loss: 0.699752\n",
            "Train Epoch: 1 | Batch Status: 780/92650 (27%) | Loss: 0.698052\n",
            "Train Epoch: 1 | Batch Status: 800/92650 (28%) | Loss: 0.692715\n",
            "Train Epoch: 1 | Batch Status: 820/92650 (28%) | Loss: 0.692681\n",
            "Train Epoch: 1 | Batch Status: 840/92650 (29%) | Loss: 0.693050\n",
            "Train Epoch: 1 | Batch Status: 860/92650 (30%) | Loss: 0.673573\n",
            "Train Epoch: 1 | Batch Status: 880/92650 (30%) | Loss: 0.700486\n",
            "Train Epoch: 1 | Batch Status: 900/92650 (31%) | Loss: 0.688046\n",
            "Train Epoch: 1 | Batch Status: 920/92650 (32%) | Loss: 0.687957\n",
            "Train Epoch: 1 | Batch Status: 940/92650 (32%) | Loss: 0.700577\n",
            "Train Epoch: 1 | Batch Status: 960/92650 (33%) | Loss: 0.697588\n",
            "Train Epoch: 1 | Batch Status: 980/92650 (34%) | Loss: 0.689954\n",
            "Train Epoch: 1 | Batch Status: 1000/92650 (35%) | Loss: 0.696793\n",
            "Train Epoch: 1 | Batch Status: 1020/92650 (35%) | Loss: 0.683552\n",
            "Train Epoch: 1 | Batch Status: 1040/92650 (36%) | Loss: 0.695246\n",
            "Train Epoch: 1 | Batch Status: 1060/92650 (37%) | Loss: 0.700214\n",
            "Train Epoch: 1 | Batch Status: 1080/92650 (37%) | Loss: 0.688874\n",
            "Train Epoch: 1 | Batch Status: 1100/92650 (38%) | Loss: 0.682441\n",
            "Train Epoch: 1 | Batch Status: 1120/92650 (39%) | Loss: 0.696268\n",
            "Train Epoch: 1 | Batch Status: 1140/92650 (39%) | Loss: 0.691544\n",
            "Train Epoch: 1 | Batch Status: 1160/92650 (40%) | Loss: 0.688823\n",
            "Train Epoch: 1 | Batch Status: 1180/92650 (41%) | Loss: 0.686914\n",
            "Train Epoch: 1 | Batch Status: 1200/92650 (41%) | Loss: 0.676204\n",
            "Train Epoch: 1 | Batch Status: 1220/92650 (42%) | Loss: 0.681350\n",
            "Train Epoch: 1 | Batch Status: 1240/92650 (43%) | Loss: 0.695741\n",
            "Train Epoch: 1 | Batch Status: 1260/92650 (44%) | Loss: 0.707276\n",
            "Train Epoch: 1 | Batch Status: 1280/92650 (44%) | Loss: 0.689696\n",
            "Train Epoch: 1 | Batch Status: 1300/92650 (45%) | Loss: 0.691158\n",
            "Train Epoch: 1 | Batch Status: 1320/92650 (46%) | Loss: 0.693319\n",
            "Train Epoch: 1 | Batch Status: 1340/92650 (46%) | Loss: 0.694398\n",
            "Train Epoch: 1 | Batch Status: 1360/92650 (47%) | Loss: 0.687535\n",
            "Train Epoch: 1 | Batch Status: 1380/92650 (48%) | Loss: 0.692685\n",
            "Train Epoch: 1 | Batch Status: 1400/92650 (48%) | Loss: 0.696824\n",
            "Train Epoch: 1 | Batch Status: 1420/92650 (49%) | Loss: 0.700253\n",
            "Train Epoch: 1 | Batch Status: 1440/92650 (50%) | Loss: 0.698720\n",
            "Train Epoch: 1 | Batch Status: 1460/92650 (50%) | Loss: 0.702492\n",
            "Train Epoch: 1 | Batch Status: 1480/92650 (51%) | Loss: 0.693607\n",
            "Train Epoch: 1 | Batch Status: 1500/92650 (52%) | Loss: 0.691430\n",
            "Train Epoch: 1 | Batch Status: 1520/92650 (52%) | Loss: 0.685078\n",
            "Train Epoch: 1 | Batch Status: 1540/92650 (53%) | Loss: 0.679145\n",
            "Train Epoch: 1 | Batch Status: 1560/92650 (54%) | Loss: 0.685732\n",
            "Train Epoch: 1 | Batch Status: 1580/92650 (55%) | Loss: 0.685721\n",
            "Train Epoch: 1 | Batch Status: 1600/92650 (55%) | Loss: 0.678241\n",
            "Train Epoch: 1 | Batch Status: 1620/92650 (56%) | Loss: 0.691248\n",
            "Train Epoch: 1 | Batch Status: 1640/92650 (57%) | Loss: 0.689912\n",
            "Train Epoch: 1 | Batch Status: 1660/92650 (57%) | Loss: 0.690270\n",
            "Train Epoch: 1 | Batch Status: 1680/92650 (58%) | Loss: 0.691084\n",
            "Train Epoch: 1 | Batch Status: 1700/92650 (59%) | Loss: 0.694668\n",
            "Train Epoch: 1 | Batch Status: 1720/92650 (59%) | Loss: 0.686201\n",
            "Train Epoch: 1 | Batch Status: 1740/92650 (60%) | Loss: 0.685427\n",
            "Train Epoch: 1 | Batch Status: 1760/92650 (61%) | Loss: 0.695778\n",
            "Train Epoch: 1 | Batch Status: 1780/92650 (61%) | Loss: 0.689926\n",
            "Train Epoch: 1 | Batch Status: 1800/92650 (62%) | Loss: 0.691664\n",
            "Train Epoch: 1 | Batch Status: 1820/92650 (63%) | Loss: 0.698599\n",
            "Train Epoch: 1 | Batch Status: 1840/92650 (64%) | Loss: 0.676681\n",
            "Train Epoch: 1 | Batch Status: 1860/92650 (64%) | Loss: 0.704784\n",
            "Train Epoch: 1 | Batch Status: 1880/92650 (65%) | Loss: 0.663482\n",
            "Train Epoch: 1 | Batch Status: 1900/92650 (66%) | Loss: 0.710980\n",
            "Train Epoch: 1 | Batch Status: 1920/92650 (66%) | Loss: 0.678831\n",
            "Train Epoch: 1 | Batch Status: 1940/92650 (67%) | Loss: 0.686621\n",
            "Train Epoch: 1 | Batch Status: 1960/92650 (68%) | Loss: 0.676816\n",
            "Train Epoch: 1 | Batch Status: 1980/92650 (68%) | Loss: 0.694498\n",
            "Train Epoch: 1 | Batch Status: 2000/92650 (69%) | Loss: 0.688790\n",
            "Train Epoch: 1 | Batch Status: 2020/92650 (70%) | Loss: 0.698616\n",
            "Train Epoch: 1 | Batch Status: 2040/92650 (70%) | Loss: 0.700776\n",
            "Train Epoch: 1 | Batch Status: 2060/92650 (71%) | Loss: 0.693174\n",
            "Train Epoch: 1 | Batch Status: 2080/92650 (72%) | Loss: 0.706212\n",
            "Train Epoch: 1 | Batch Status: 2100/92650 (73%) | Loss: 0.681137\n",
            "Train Epoch: 1 | Batch Status: 2120/92650 (73%) | Loss: 0.691333\n",
            "Train Epoch: 1 | Batch Status: 2140/92650 (74%) | Loss: 0.691239\n",
            "Train Epoch: 1 | Batch Status: 2160/92650 (75%) | Loss: 0.700695\n",
            "Train Epoch: 1 | Batch Status: 2180/92650 (75%) | Loss: 0.700597\n",
            "Train Epoch: 1 | Batch Status: 2200/92650 (76%) | Loss: 0.687349\n",
            "Train Epoch: 1 | Batch Status: 2220/92650 (77%) | Loss: 0.695312\n",
            "Train Epoch: 1 | Batch Status: 2240/92650 (77%) | Loss: 0.681992\n",
            "Train Epoch: 1 | Batch Status: 2260/92650 (78%) | Loss: 0.708343\n",
            "Train Epoch: 1 | Batch Status: 2280/92650 (79%) | Loss: 0.694158\n",
            "Train Epoch: 1 | Batch Status: 2300/92650 (79%) | Loss: 0.694113\n",
            "Train Epoch: 1 | Batch Status: 2320/92650 (80%) | Loss: 0.686029\n",
            "Train Epoch: 1 | Batch Status: 2340/92650 (81%) | Loss: 0.681311\n",
            "Train Epoch: 1 | Batch Status: 2360/92650 (81%) | Loss: 0.679195\n",
            "Train Epoch: 1 | Batch Status: 2380/92650 (82%) | Loss: 0.701019\n",
            "Train Epoch: 1 | Batch Status: 2400/92650 (83%) | Loss: 0.691193\n",
            "Train Epoch: 1 | Batch Status: 2420/92650 (84%) | Loss: 0.697527\n",
            "Train Epoch: 1 | Batch Status: 2440/92650 (84%) | Loss: 0.700872\n",
            "Train Epoch: 1 | Batch Status: 2460/92650 (85%) | Loss: 0.681499\n",
            "Train Epoch: 1 | Batch Status: 2480/92650 (86%) | Loss: 0.711176\n",
            "Train Epoch: 1 | Batch Status: 2500/92650 (86%) | Loss: 0.701934\n",
            "Train Epoch: 1 | Batch Status: 2520/92650 (87%) | Loss: 0.707934\n",
            "Train Epoch: 1 | Batch Status: 2540/92650 (88%) | Loss: 0.698184\n",
            "Train Epoch: 1 | Batch Status: 2560/92650 (88%) | Loss: 0.691890\n",
            "Train Epoch: 1 | Batch Status: 2580/92650 (89%) | Loss: 0.690384\n",
            "Train Epoch: 1 | Batch Status: 2600/92650 (90%) | Loss: 0.688649\n",
            "Train Epoch: 1 | Batch Status: 2620/92650 (90%) | Loss: 0.671621\n",
            "Train Epoch: 1 | Batch Status: 2640/92650 (91%) | Loss: 0.702894\n",
            "Train Epoch: 1 | Batch Status: 2660/92650 (92%) | Loss: 0.688891\n",
            "Train Epoch: 1 | Batch Status: 2680/92650 (93%) | Loss: 0.689070\n",
            "Train Epoch: 1 | Batch Status: 2700/92650 (93%) | Loss: 0.703132\n",
            "Train Epoch: 1 | Batch Status: 2720/92650 (94%) | Loss: 0.694136\n",
            "Train Epoch: 1 | Batch Status: 2740/92650 (95%) | Loss: 0.693682\n",
            "Train Epoch: 1 | Batch Status: 2760/92650 (95%) | Loss: 0.694261\n",
            "Train Epoch: 1 | Batch Status: 2780/92650 (96%) | Loss: 0.692742\n",
            "Train Epoch: 1 | Batch Status: 2800/92650 (97%) | Loss: 0.686569\n",
            "Train Epoch: 1 | Batch Status: 2820/92650 (97%) | Loss: 0.695370\n",
            "Train Epoch: 1 | Batch Status: 2840/92650 (98%) | Loss: 0.693742\n",
            "Train Epoch: 1 | Batch Status: 2860/92650 (99%) | Loss: 0.692490\n",
            "Train Epoch: 1 | Batch Status: 2880/92650 (99%) | Loss: 0.692538\n",
            "Train Epoch: 2 | Batch Status: 0/92650 (0%) | Loss: 0.696330\n",
            "Train Epoch: 2 | Batch Status: 20/92650 (1%) | Loss: 0.692695\n",
            "Train Epoch: 2 | Batch Status: 40/92650 (1%) | Loss: 0.700635\n",
            "Train Epoch: 2 | Batch Status: 60/92650 (2%) | Loss: 0.701252\n",
            "Train Epoch: 2 | Batch Status: 80/92650 (3%) | Loss: 0.694699\n",
            "Train Epoch: 2 | Batch Status: 100/92650 (3%) | Loss: 0.695854\n",
            "Train Epoch: 2 | Batch Status: 120/92650 (4%) | Loss: 0.685997\n",
            "Train Epoch: 2 | Batch Status: 140/92650 (5%) | Loss: 0.683710\n",
            "Train Epoch: 2 | Batch Status: 160/92650 (6%) | Loss: 0.697585\n",
            "Train Epoch: 2 | Batch Status: 180/92650 (6%) | Loss: 0.693156\n",
            "Train Epoch: 2 | Batch Status: 200/92650 (7%) | Loss: 0.693745\n",
            "Train Epoch: 2 | Batch Status: 220/92650 (8%) | Loss: 0.694818\n",
            "Train Epoch: 2 | Batch Status: 240/92650 (8%) | Loss: 0.694598\n",
            "Train Epoch: 2 | Batch Status: 260/92650 (9%) | Loss: 0.685401\n",
            "Train Epoch: 2 | Batch Status: 280/92650 (10%) | Loss: 0.691978\n",
            "Train Epoch: 2 | Batch Status: 300/92650 (10%) | Loss: 0.681711\n",
            "Train Epoch: 2 | Batch Status: 320/92650 (11%) | Loss: 0.691515\n",
            "Train Epoch: 2 | Batch Status: 340/92650 (12%) | Loss: 0.688316\n",
            "Train Epoch: 2 | Batch Status: 360/92650 (12%) | Loss: 0.689909\n",
            "Train Epoch: 2 | Batch Status: 380/92650 (13%) | Loss: 0.702328\n",
            "Train Epoch: 2 | Batch Status: 400/92650 (14%) | Loss: 0.700446\n",
            "Train Epoch: 2 | Batch Status: 420/92650 (15%) | Loss: 0.701181\n",
            "Train Epoch: 2 | Batch Status: 440/92650 (15%) | Loss: 0.695071\n",
            "Train Epoch: 2 | Batch Status: 460/92650 (16%) | Loss: 0.691758\n",
            "Train Epoch: 2 | Batch Status: 480/92650 (17%) | Loss: 0.691803\n",
            "Train Epoch: 2 | Batch Status: 500/92650 (17%) | Loss: 0.682971\n",
            "Train Epoch: 2 | Batch Status: 520/92650 (18%) | Loss: 0.709390\n",
            "Train Epoch: 2 | Batch Status: 540/92650 (19%) | Loss: 0.688219\n",
            "Train Epoch: 2 | Batch Status: 560/92650 (19%) | Loss: 0.689386\n",
            "Train Epoch: 2 | Batch Status: 580/92650 (20%) | Loss: 0.693034\n",
            "Train Epoch: 2 | Batch Status: 600/92650 (21%) | Loss: 0.693636\n",
            "Train Epoch: 2 | Batch Status: 620/92650 (21%) | Loss: 0.696401\n",
            "Train Epoch: 2 | Batch Status: 640/92650 (22%) | Loss: 0.662697\n",
            "Train Epoch: 2 | Batch Status: 660/92650 (23%) | Loss: 0.685399\n",
            "Train Epoch: 2 | Batch Status: 680/92650 (23%) | Loss: 0.705825\n",
            "Train Epoch: 2 | Batch Status: 700/92650 (24%) | Loss: 0.692798\n",
            "Train Epoch: 2 | Batch Status: 720/92650 (25%) | Loss: 0.690898\n",
            "Train Epoch: 2 | Batch Status: 740/92650 (26%) | Loss: 0.692704\n",
            "Train Epoch: 2 | Batch Status: 760/92650 (26%) | Loss: 0.688657\n",
            "Train Epoch: 2 | Batch Status: 780/92650 (27%) | Loss: 0.695409\n",
            "Train Epoch: 2 | Batch Status: 800/92650 (28%) | Loss: 0.689466\n",
            "Train Epoch: 2 | Batch Status: 820/92650 (28%) | Loss: 0.687128\n",
            "Train Epoch: 2 | Batch Status: 840/92650 (29%) | Loss: 0.694520\n",
            "Train Epoch: 2 | Batch Status: 860/92650 (30%) | Loss: 0.690531\n",
            "Train Epoch: 2 | Batch Status: 880/92650 (30%) | Loss: 0.690591\n",
            "Train Epoch: 2 | Batch Status: 900/92650 (31%) | Loss: 0.692762\n",
            "Train Epoch: 2 | Batch Status: 920/92650 (32%) | Loss: 0.673452\n",
            "Train Epoch: 2 | Batch Status: 940/92650 (32%) | Loss: 0.680799\n",
            "Train Epoch: 2 | Batch Status: 960/92650 (33%) | Loss: 0.696112\n",
            "Train Epoch: 2 | Batch Status: 980/92650 (34%) | Loss: 0.696304\n",
            "Train Epoch: 2 | Batch Status: 1000/92650 (35%) | Loss: 0.692914\n",
            "Train Epoch: 2 | Batch Status: 1020/92650 (35%) | Loss: 0.687168\n",
            "Train Epoch: 2 | Batch Status: 1040/92650 (36%) | Loss: 0.687358\n",
            "Train Epoch: 2 | Batch Status: 1060/92650 (37%) | Loss: 0.708811\n",
            "Train Epoch: 2 | Batch Status: 1080/92650 (37%) | Loss: 0.682146\n",
            "Train Epoch: 2 | Batch Status: 1100/92650 (38%) | Loss: 0.675891\n",
            "Train Epoch: 2 | Batch Status: 1120/92650 (39%) | Loss: 0.694959\n",
            "Train Epoch: 2 | Batch Status: 1140/92650 (39%) | Loss: 0.706895\n",
            "Train Epoch: 2 | Batch Status: 1160/92650 (40%) | Loss: 0.693631\n",
            "Train Epoch: 2 | Batch Status: 1180/92650 (41%) | Loss: 0.684953\n",
            "Train Epoch: 2 | Batch Status: 1200/92650 (41%) | Loss: 0.686462\n",
            "Train Epoch: 2 | Batch Status: 1220/92650 (42%) | Loss: 0.690747\n",
            "Train Epoch: 2 | Batch Status: 1240/92650 (43%) | Loss: 0.682629\n",
            "Train Epoch: 2 | Batch Status: 1260/92650 (44%) | Loss: 0.685729\n",
            "Train Epoch: 2 | Batch Status: 1280/92650 (44%) | Loss: 0.700057\n",
            "Train Epoch: 2 | Batch Status: 1300/92650 (45%) | Loss: 0.694776\n",
            "Train Epoch: 2 | Batch Status: 1320/92650 (46%) | Loss: 0.682311\n",
            "Train Epoch: 2 | Batch Status: 1340/92650 (46%) | Loss: 0.676285\n",
            "Train Epoch: 2 | Batch Status: 1360/92650 (47%) | Loss: 0.696412\n",
            "Train Epoch: 2 | Batch Status: 1380/92650 (48%) | Loss: 0.688882\n",
            "Train Epoch: 2 | Batch Status: 1400/92650 (48%) | Loss: 0.713332\n",
            "Train Epoch: 2 | Batch Status: 1420/92650 (49%) | Loss: 0.697056\n",
            "Train Epoch: 2 | Batch Status: 1440/92650 (50%) | Loss: 0.691705\n",
            "Train Epoch: 2 | Batch Status: 1460/92650 (50%) | Loss: 0.685232\n",
            "Train Epoch: 2 | Batch Status: 1480/92650 (51%) | Loss: 0.679757\n",
            "Train Epoch: 2 | Batch Status: 1500/92650 (52%) | Loss: 0.688774\n",
            "Train Epoch: 2 | Batch Status: 1520/92650 (52%) | Loss: 0.705938\n",
            "Train Epoch: 2 | Batch Status: 1540/92650 (53%) | Loss: 0.700886\n",
            "Train Epoch: 2 | Batch Status: 1560/92650 (54%) | Loss: 0.697604\n",
            "Train Epoch: 2 | Batch Status: 1580/92650 (55%) | Loss: 0.693157\n",
            "Train Epoch: 2 | Batch Status: 1600/92650 (55%) | Loss: 0.693561\n",
            "Train Epoch: 2 | Batch Status: 1620/92650 (56%) | Loss: 0.698607\n",
            "Train Epoch: 2 | Batch Status: 1640/92650 (57%) | Loss: 0.705003\n",
            "Train Epoch: 2 | Batch Status: 1660/92650 (57%) | Loss: 0.687786\n",
            "Train Epoch: 2 | Batch Status: 1680/92650 (58%) | Loss: 0.694591\n",
            "Train Epoch: 2 | Batch Status: 1700/92650 (59%) | Loss: 0.692924\n",
            "Train Epoch: 2 | Batch Status: 1720/92650 (59%) | Loss: 0.692849\n",
            "Train Epoch: 2 | Batch Status: 1740/92650 (60%) | Loss: 0.699052\n",
            "Train Epoch: 2 | Batch Status: 1760/92650 (61%) | Loss: 0.691967\n",
            "Train Epoch: 2 | Batch Status: 1780/92650 (61%) | Loss: 0.696442\n",
            "Train Epoch: 2 | Batch Status: 1800/92650 (62%) | Loss: 0.694231\n",
            "Train Epoch: 2 | Batch Status: 1820/92650 (63%) | Loss: 0.693172\n",
            "Train Epoch: 2 | Batch Status: 1840/92650 (64%) | Loss: 0.693542\n",
            "Train Epoch: 2 | Batch Status: 1860/92650 (64%) | Loss: 0.692051\n",
            "Train Epoch: 2 | Batch Status: 1880/92650 (65%) | Loss: 0.689639\n",
            "Train Epoch: 2 | Batch Status: 1900/92650 (66%) | Loss: 0.698070\n",
            "Train Epoch: 2 | Batch Status: 1920/92650 (66%) | Loss: 0.687075\n",
            "Train Epoch: 2 | Batch Status: 1940/92650 (67%) | Loss: 0.694124\n",
            "Train Epoch: 2 | Batch Status: 1960/92650 (68%) | Loss: 0.706744\n",
            "Train Epoch: 2 | Batch Status: 1980/92650 (68%) | Loss: 0.691527\n",
            "Train Epoch: 2 | Batch Status: 2000/92650 (69%) | Loss: 0.685988\n",
            "Train Epoch: 2 | Batch Status: 2020/92650 (70%) | Loss: 0.701810\n",
            "Train Epoch: 2 | Batch Status: 2040/92650 (70%) | Loss: 0.692667\n",
            "Train Epoch: 2 | Batch Status: 2060/92650 (71%) | Loss: 0.693924\n",
            "Train Epoch: 2 | Batch Status: 2080/92650 (72%) | Loss: 0.695166\n",
            "Train Epoch: 2 | Batch Status: 2100/92650 (73%) | Loss: 0.699984\n",
            "Train Epoch: 2 | Batch Status: 2120/92650 (73%) | Loss: 0.691506\n",
            "Train Epoch: 2 | Batch Status: 2140/92650 (74%) | Loss: 0.691243\n",
            "Train Epoch: 2 | Batch Status: 2160/92650 (75%) | Loss: 0.685772\n",
            "Train Epoch: 2 | Batch Status: 2180/92650 (75%) | Loss: 0.698151\n",
            "Train Epoch: 2 | Batch Status: 2200/92650 (76%) | Loss: 0.699597\n",
            "Train Epoch: 2 | Batch Status: 2220/92650 (77%) | Loss: 0.696914\n",
            "Train Epoch: 2 | Batch Status: 2240/92650 (77%) | Loss: 0.685868\n",
            "Train Epoch: 2 | Batch Status: 2260/92650 (78%) | Loss: 0.677845\n",
            "Train Epoch: 2 | Batch Status: 2280/92650 (79%) | Loss: 0.688750\n",
            "Train Epoch: 2 | Batch Status: 2300/92650 (79%) | Loss: 0.677145\n",
            "Train Epoch: 2 | Batch Status: 2320/92650 (80%) | Loss: 0.696521\n",
            "Train Epoch: 2 | Batch Status: 2340/92650 (81%) | Loss: 0.682834\n",
            "Train Epoch: 2 | Batch Status: 2360/92650 (81%) | Loss: 0.696230\n",
            "Train Epoch: 2 | Batch Status: 2380/92650 (82%) | Loss: 0.709557\n",
            "Train Epoch: 2 | Batch Status: 2400/92650 (83%) | Loss: 0.699510\n",
            "Train Epoch: 2 | Batch Status: 2420/92650 (84%) | Loss: 0.691390\n",
            "Train Epoch: 2 | Batch Status: 2440/92650 (84%) | Loss: 0.695071\n",
            "Train Epoch: 2 | Batch Status: 2460/92650 (85%) | Loss: 0.699528\n",
            "Train Epoch: 2 | Batch Status: 2480/92650 (86%) | Loss: 0.692850\n",
            "Train Epoch: 2 | Batch Status: 2500/92650 (86%) | Loss: 0.685471\n",
            "Train Epoch: 2 | Batch Status: 2520/92650 (87%) | Loss: 0.685234\n",
            "Train Epoch: 2 | Batch Status: 2540/92650 (88%) | Loss: 0.688781\n",
            "Train Epoch: 2 | Batch Status: 2560/92650 (88%) | Loss: 0.688756\n",
            "Train Epoch: 2 | Batch Status: 2580/92650 (89%) | Loss: 0.699203\n",
            "Train Epoch: 2 | Batch Status: 2600/92650 (90%) | Loss: 0.692839\n",
            "Train Epoch: 2 | Batch Status: 2620/92650 (90%) | Loss: 0.688182\n",
            "Train Epoch: 2 | Batch Status: 2640/92650 (91%) | Loss: 0.681615\n",
            "Train Epoch: 2 | Batch Status: 2660/92650 (92%) | Loss: 0.675317\n",
            "Train Epoch: 2 | Batch Status: 2680/92650 (93%) | Loss: 0.703635\n",
            "Train Epoch: 2 | Batch Status: 2700/92650 (93%) | Loss: 0.694546\n",
            "Train Epoch: 2 | Batch Status: 2720/92650 (94%) | Loss: 0.689028\n",
            "Train Epoch: 2 | Batch Status: 2740/92650 (95%) | Loss: 0.694058\n",
            "Train Epoch: 2 | Batch Status: 2760/92650 (95%) | Loss: 0.691324\n",
            "Train Epoch: 2 | Batch Status: 2780/92650 (96%) | Loss: 0.697191\n",
            "Train Epoch: 2 | Batch Status: 2800/92650 (97%) | Loss: 0.688746\n",
            "Train Epoch: 2 | Batch Status: 2820/92650 (97%) | Loss: 0.701172\n",
            "Train Epoch: 2 | Batch Status: 2840/92650 (98%) | Loss: 0.688953\n",
            "Train Epoch: 2 | Batch Status: 2860/92650 (99%) | Loss: 0.690294\n",
            "Train Epoch: 2 | Batch Status: 2880/92650 (99%) | Loss: 0.694206\n",
            "Train Epoch: 3 | Batch Status: 0/92650 (0%) | Loss: 0.698935\n",
            "Train Epoch: 3 | Batch Status: 20/92650 (1%) | Loss: 0.688292\n",
            "Train Epoch: 3 | Batch Status: 40/92650 (1%) | Loss: 0.690177\n",
            "Train Epoch: 3 | Batch Status: 60/92650 (2%) | Loss: 0.699222\n",
            "Train Epoch: 3 | Batch Status: 80/92650 (3%) | Loss: 0.689828\n",
            "Train Epoch: 3 | Batch Status: 100/92650 (3%) | Loss: 0.692669\n",
            "Train Epoch: 3 | Batch Status: 120/92650 (4%) | Loss: 0.689647\n",
            "Train Epoch: 3 | Batch Status: 140/92650 (5%) | Loss: 0.690780\n",
            "Train Epoch: 3 | Batch Status: 160/92650 (6%) | Loss: 0.696957\n",
            "Train Epoch: 3 | Batch Status: 180/92650 (6%) | Loss: 0.689039\n",
            "Train Epoch: 3 | Batch Status: 200/92650 (7%) | Loss: 0.691537\n",
            "Train Epoch: 3 | Batch Status: 220/92650 (8%) | Loss: 0.695593\n",
            "Train Epoch: 3 | Batch Status: 240/92650 (8%) | Loss: 0.692736\n",
            "Train Epoch: 3 | Batch Status: 260/92650 (9%) | Loss: 0.676747\n",
            "Train Epoch: 3 | Batch Status: 280/92650 (10%) | Loss: 0.691485\n",
            "Train Epoch: 3 | Batch Status: 300/92650 (10%) | Loss: 0.673211\n",
            "Train Epoch: 3 | Batch Status: 320/92650 (11%) | Loss: 0.686239\n",
            "Train Epoch: 3 | Batch Status: 340/92650 (12%) | Loss: 0.701253\n",
            "Train Epoch: 3 | Batch Status: 360/92650 (12%) | Loss: 0.691199\n",
            "Train Epoch: 3 | Batch Status: 380/92650 (13%) | Loss: 0.687988\n",
            "Train Epoch: 3 | Batch Status: 400/92650 (14%) | Loss: 0.696767\n",
            "Train Epoch: 3 | Batch Status: 420/92650 (15%) | Loss: 0.697589\n",
            "Train Epoch: 3 | Batch Status: 440/92650 (15%) | Loss: 0.686091\n",
            "Train Epoch: 3 | Batch Status: 460/92650 (16%) | Loss: 0.716137\n",
            "Train Epoch: 3 | Batch Status: 480/92650 (17%) | Loss: 0.691426\n",
            "Train Epoch: 3 | Batch Status: 500/92650 (17%) | Loss: 0.695111\n",
            "Train Epoch: 3 | Batch Status: 520/92650 (18%) | Loss: 0.693605\n",
            "Train Epoch: 3 | Batch Status: 540/92650 (19%) | Loss: 0.693956\n",
            "Train Epoch: 3 | Batch Status: 560/92650 (19%) | Loss: 0.697103\n",
            "Train Epoch: 3 | Batch Status: 580/92650 (20%) | Loss: 0.698270\n",
            "Train Epoch: 3 | Batch Status: 600/92650 (21%) | Loss: 0.689117\n",
            "Train Epoch: 3 | Batch Status: 620/92650 (21%) | Loss: 0.702813\n",
            "Train Epoch: 3 | Batch Status: 640/92650 (22%) | Loss: 0.700946\n",
            "Train Epoch: 3 | Batch Status: 660/92650 (23%) | Loss: 0.680229\n",
            "Train Epoch: 3 | Batch Status: 680/92650 (23%) | Loss: 0.699715\n",
            "Train Epoch: 3 | Batch Status: 700/92650 (24%) | Loss: 0.683278\n",
            "Train Epoch: 3 | Batch Status: 720/92650 (25%) | Loss: 0.688993\n",
            "Train Epoch: 3 | Batch Status: 740/92650 (26%) | Loss: 0.703237\n",
            "Train Epoch: 3 | Batch Status: 760/92650 (26%) | Loss: 0.695876\n",
            "Train Epoch: 3 | Batch Status: 780/92650 (27%) | Loss: 0.692818\n",
            "Train Epoch: 3 | Batch Status: 800/92650 (28%) | Loss: 0.688725\n",
            "Train Epoch: 3 | Batch Status: 820/92650 (28%) | Loss: 0.704390\n",
            "Train Epoch: 3 | Batch Status: 840/92650 (29%) | Loss: 0.679807\n",
            "Train Epoch: 3 | Batch Status: 860/92650 (30%) | Loss: 0.689027\n",
            "Train Epoch: 3 | Batch Status: 880/92650 (30%) | Loss: 0.697628\n",
            "Train Epoch: 3 | Batch Status: 900/92650 (31%) | Loss: 0.690301\n",
            "Train Epoch: 3 | Batch Status: 920/92650 (32%) | Loss: 0.688466\n",
            "Train Epoch: 3 | Batch Status: 940/92650 (32%) | Loss: 0.700903\n",
            "Train Epoch: 3 | Batch Status: 960/92650 (33%) | Loss: 0.705488\n",
            "Train Epoch: 3 | Batch Status: 980/92650 (34%) | Loss: 0.695085\n",
            "Train Epoch: 3 | Batch Status: 1000/92650 (35%) | Loss: 0.693709\n",
            "Train Epoch: 3 | Batch Status: 1020/92650 (35%) | Loss: 0.687267\n",
            "Train Epoch: 3 | Batch Status: 1040/92650 (36%) | Loss: 0.693382\n",
            "Train Epoch: 3 | Batch Status: 1060/92650 (37%) | Loss: 0.704914\n",
            "Train Epoch: 3 | Batch Status: 1080/92650 (37%) | Loss: 0.695599\n",
            "Train Epoch: 3 | Batch Status: 1100/92650 (38%) | Loss: 0.686170\n",
            "Train Epoch: 3 | Batch Status: 1120/92650 (39%) | Loss: 0.689390\n",
            "Train Epoch: 3 | Batch Status: 1140/92650 (39%) | Loss: 0.688214\n",
            "Train Epoch: 3 | Batch Status: 1160/92650 (40%) | Loss: 0.690765\n",
            "Train Epoch: 3 | Batch Status: 1180/92650 (41%) | Loss: 0.694875\n",
            "Train Epoch: 3 | Batch Status: 1200/92650 (41%) | Loss: 0.691482\n",
            "Train Epoch: 3 | Batch Status: 1220/92650 (42%) | Loss: 0.690700\n",
            "Train Epoch: 3 | Batch Status: 1240/92650 (43%) | Loss: 0.678815\n",
            "Train Epoch: 3 | Batch Status: 1260/92650 (44%) | Loss: 0.685212\n",
            "Train Epoch: 3 | Batch Status: 1280/92650 (44%) | Loss: 0.694857\n",
            "Train Epoch: 3 | Batch Status: 1300/92650 (45%) | Loss: 0.696595\n",
            "Train Epoch: 3 | Batch Status: 1320/92650 (46%) | Loss: 0.693399\n",
            "Train Epoch: 3 | Batch Status: 1340/92650 (46%) | Loss: 0.689716\n",
            "Train Epoch: 3 | Batch Status: 1360/92650 (47%) | Loss: 0.692699\n",
            "Train Epoch: 3 | Batch Status: 1380/92650 (48%) | Loss: 0.694696\n",
            "Train Epoch: 3 | Batch Status: 1400/92650 (48%) | Loss: 0.690185\n",
            "Train Epoch: 3 | Batch Status: 1420/92650 (49%) | Loss: 0.687614\n",
            "Train Epoch: 3 | Batch Status: 1440/92650 (50%) | Loss: 0.707412\n",
            "Train Epoch: 3 | Batch Status: 1460/92650 (50%) | Loss: 0.682787\n",
            "Train Epoch: 3 | Batch Status: 1480/92650 (51%) | Loss: 0.699025\n",
            "Train Epoch: 3 | Batch Status: 1500/92650 (52%) | Loss: 0.687346\n",
            "Train Epoch: 3 | Batch Status: 1520/92650 (52%) | Loss: 0.688802\n",
            "Train Epoch: 3 | Batch Status: 1540/92650 (53%) | Loss: 0.694261\n",
            "Train Epoch: 3 | Batch Status: 1560/92650 (54%) | Loss: 0.689436\n",
            "Train Epoch: 3 | Batch Status: 1580/92650 (55%) | Loss: 0.693544\n",
            "Train Epoch: 3 | Batch Status: 1600/92650 (55%) | Loss: 0.691837\n",
            "Train Epoch: 3 | Batch Status: 1620/92650 (56%) | Loss: 0.694181\n",
            "Train Epoch: 3 | Batch Status: 1640/92650 (57%) | Loss: 0.682748\n",
            "Train Epoch: 3 | Batch Status: 1660/92650 (57%) | Loss: 0.684478\n",
            "Train Epoch: 3 | Batch Status: 1680/92650 (58%) | Loss: 0.679754\n",
            "Train Epoch: 3 | Batch Status: 1700/92650 (59%) | Loss: 0.684372\n",
            "Train Epoch: 3 | Batch Status: 1720/92650 (59%) | Loss: 0.689834\n",
            "Train Epoch: 3 | Batch Status: 1740/92650 (60%) | Loss: 0.689888\n",
            "Train Epoch: 3 | Batch Status: 1760/92650 (61%) | Loss: 0.678773\n",
            "Train Epoch: 3 | Batch Status: 1780/92650 (61%) | Loss: 0.683601\n",
            "Train Epoch: 3 | Batch Status: 1800/92650 (62%) | Loss: 0.680903\n",
            "Train Epoch: 3 | Batch Status: 1820/92650 (63%) | Loss: 0.682122\n",
            "Train Epoch: 3 | Batch Status: 1840/92650 (64%) | Loss: 0.693796\n",
            "Train Epoch: 3 | Batch Status: 1860/92650 (64%) | Loss: 0.677577\n",
            "Train Epoch: 3 | Batch Status: 1880/92650 (65%) | Loss: 0.716879\n",
            "Train Epoch: 3 | Batch Status: 1900/92650 (66%) | Loss: 0.691205\n",
            "Train Epoch: 3 | Batch Status: 1920/92650 (66%) | Loss: 0.691949\n",
            "Train Epoch: 3 | Batch Status: 1940/92650 (67%) | Loss: 0.694770\n",
            "Train Epoch: 3 | Batch Status: 1960/92650 (68%) | Loss: 0.687285\n",
            "Train Epoch: 3 | Batch Status: 1980/92650 (68%) | Loss: 0.685308\n",
            "Train Epoch: 3 | Batch Status: 2000/92650 (69%) | Loss: 0.695186\n",
            "Train Epoch: 3 | Batch Status: 2020/92650 (70%) | Loss: 0.692774\n",
            "Train Epoch: 3 | Batch Status: 2040/92650 (70%) | Loss: 0.683628\n",
            "Train Epoch: 3 | Batch Status: 2060/92650 (71%) | Loss: 0.682659\n",
            "Train Epoch: 3 | Batch Status: 2080/92650 (72%) | Loss: 0.701165\n",
            "Train Epoch: 3 | Batch Status: 2100/92650 (73%) | Loss: 0.691234\n",
            "Train Epoch: 3 | Batch Status: 2120/92650 (73%) | Loss: 0.676214\n",
            "Train Epoch: 3 | Batch Status: 2140/92650 (74%) | Loss: 0.682513\n",
            "Train Epoch: 3 | Batch Status: 2160/92650 (75%) | Loss: 0.709069\n",
            "Train Epoch: 3 | Batch Status: 2180/92650 (75%) | Loss: 0.693872\n",
            "Train Epoch: 3 | Batch Status: 2200/92650 (76%) | Loss: 0.701616\n",
            "Train Epoch: 3 | Batch Status: 2220/92650 (77%) | Loss: 0.692578\n",
            "Train Epoch: 3 | Batch Status: 2240/92650 (77%) | Loss: 0.693173\n",
            "Train Epoch: 3 | Batch Status: 2260/92650 (78%) | Loss: 0.691880\n",
            "Train Epoch: 3 | Batch Status: 2280/92650 (79%) | Loss: 0.690105\n",
            "Train Epoch: 3 | Batch Status: 2300/92650 (79%) | Loss: 0.684291\n",
            "Train Epoch: 3 | Batch Status: 2320/92650 (80%) | Loss: 0.699559\n",
            "Train Epoch: 3 | Batch Status: 2340/92650 (81%) | Loss: 0.678788\n",
            "Train Epoch: 3 | Batch Status: 2360/92650 (81%) | Loss: 0.688835\n",
            "Train Epoch: 3 | Batch Status: 2380/92650 (82%) | Loss: 0.696194\n",
            "Train Epoch: 3 | Batch Status: 2400/92650 (83%) | Loss: 0.696620\n",
            "Train Epoch: 3 | Batch Status: 2420/92650 (84%) | Loss: 0.684579\n",
            "Train Epoch: 3 | Batch Status: 2440/92650 (84%) | Loss: 0.693840\n",
            "Train Epoch: 3 | Batch Status: 2460/92650 (85%) | Loss: 0.692962\n",
            "Train Epoch: 3 | Batch Status: 2480/92650 (86%) | Loss: 0.681594\n",
            "Train Epoch: 3 | Batch Status: 2500/92650 (86%) | Loss: 0.688951\n",
            "Train Epoch: 3 | Batch Status: 2520/92650 (87%) | Loss: 0.689504\n",
            "Train Epoch: 3 | Batch Status: 2540/92650 (88%) | Loss: 0.692872\n",
            "Train Epoch: 3 | Batch Status: 2560/92650 (88%) | Loss: 0.691194\n",
            "Train Epoch: 3 | Batch Status: 2580/92650 (89%) | Loss: 0.697992\n",
            "Train Epoch: 3 | Batch Status: 2600/92650 (90%) | Loss: 0.691427\n",
            "Train Epoch: 3 | Batch Status: 2620/92650 (90%) | Loss: 0.685263\n",
            "Train Epoch: 3 | Batch Status: 2640/92650 (91%) | Loss: 0.691381\n",
            "Train Epoch: 3 | Batch Status: 2660/92650 (92%) | Loss: 0.688324\n",
            "Train Epoch: 3 | Batch Status: 2680/92650 (93%) | Loss: 0.694400\n",
            "Train Epoch: 3 | Batch Status: 2700/92650 (93%) | Loss: 0.691201\n",
            "Train Epoch: 3 | Batch Status: 2720/92650 (94%) | Loss: 0.685853\n",
            "Train Epoch: 3 | Batch Status: 2740/92650 (95%) | Loss: 0.690558\n",
            "Train Epoch: 3 | Batch Status: 2760/92650 (95%) | Loss: 0.691594\n",
            "Train Epoch: 3 | Batch Status: 2780/92650 (96%) | Loss: 0.695505\n",
            "Train Epoch: 3 | Batch Status: 2800/92650 (97%) | Loss: 0.669352\n",
            "Train Epoch: 3 | Batch Status: 2820/92650 (97%) | Loss: 0.672651\n",
            "Train Epoch: 3 | Batch Status: 2840/92650 (98%) | Loss: 0.681180\n",
            "Train Epoch: 3 | Batch Status: 2860/92650 (99%) | Loss: 0.681002\n",
            "Train Epoch: 3 | Batch Status: 2880/92650 (99%) | Loss: 0.708921\n",
            "Train Epoch: 4 | Batch Status: 0/92650 (0%) | Loss: 0.699553\n",
            "Train Epoch: 4 | Batch Status: 20/92650 (1%) | Loss: 0.700844\n",
            "Train Epoch: 4 | Batch Status: 40/92650 (1%) | Loss: 0.696201\n",
            "Train Epoch: 4 | Batch Status: 60/92650 (2%) | Loss: 0.693356\n",
            "Train Epoch: 4 | Batch Status: 80/92650 (3%) | Loss: 0.694795\n",
            "Train Epoch: 4 | Batch Status: 100/92650 (3%) | Loss: 0.696782\n",
            "Train Epoch: 4 | Batch Status: 120/92650 (4%) | Loss: 0.694674\n",
            "Train Epoch: 4 | Batch Status: 140/92650 (5%) | Loss: 0.694007\n",
            "Train Epoch: 4 | Batch Status: 160/92650 (6%) | Loss: 0.693656\n",
            "Train Epoch: 4 | Batch Status: 180/92650 (6%) | Loss: 0.695672\n",
            "Train Epoch: 4 | Batch Status: 200/92650 (7%) | Loss: 0.693827\n",
            "Train Epoch: 4 | Batch Status: 220/92650 (8%) | Loss: 0.694834\n",
            "Train Epoch: 4 | Batch Status: 240/92650 (8%) | Loss: 0.683066\n",
            "Train Epoch: 4 | Batch Status: 260/92650 (9%) | Loss: 0.682188\n",
            "Train Epoch: 4 | Batch Status: 280/92650 (10%) | Loss: 0.701034\n",
            "Train Epoch: 4 | Batch Status: 300/92650 (10%) | Loss: 0.691390\n",
            "Train Epoch: 4 | Batch Status: 320/92650 (11%) | Loss: 0.679309\n",
            "Train Epoch: 4 | Batch Status: 340/92650 (12%) | Loss: 0.691494\n",
            "Train Epoch: 4 | Batch Status: 360/92650 (12%) | Loss: 0.687828\n",
            "Train Epoch: 4 | Batch Status: 380/92650 (13%) | Loss: 0.694275\n",
            "Train Epoch: 4 | Batch Status: 400/92650 (14%) | Loss: 0.692159\n",
            "Train Epoch: 4 | Batch Status: 420/92650 (15%) | Loss: 0.697304\n",
            "Train Epoch: 4 | Batch Status: 440/92650 (15%) | Loss: 0.692855\n",
            "Train Epoch: 4 | Batch Status: 460/92650 (16%) | Loss: 0.705696\n",
            "Train Epoch: 4 | Batch Status: 480/92650 (17%) | Loss: 0.693757\n",
            "Train Epoch: 4 | Batch Status: 500/92650 (17%) | Loss: 0.693148\n",
            "Train Epoch: 4 | Batch Status: 520/92650 (18%) | Loss: 0.691197\n",
            "Train Epoch: 4 | Batch Status: 540/92650 (19%) | Loss: 0.695162\n",
            "Train Epoch: 4 | Batch Status: 560/92650 (19%) | Loss: 0.710477\n",
            "Train Epoch: 4 | Batch Status: 580/92650 (20%) | Loss: 0.697256\n",
            "Train Epoch: 4 | Batch Status: 600/92650 (21%) | Loss: 0.702858\n",
            "Train Epoch: 4 | Batch Status: 620/92650 (21%) | Loss: 0.700309\n",
            "Train Epoch: 4 | Batch Status: 640/92650 (22%) | Loss: 0.688945\n",
            "Train Epoch: 4 | Batch Status: 660/92650 (23%) | Loss: 0.683548\n",
            "Train Epoch: 4 | Batch Status: 680/92650 (23%) | Loss: 0.679957\n",
            "Train Epoch: 4 | Batch Status: 700/92650 (24%) | Loss: 0.696493\n",
            "Train Epoch: 4 | Batch Status: 720/92650 (25%) | Loss: 0.692692\n",
            "Train Epoch: 4 | Batch Status: 740/92650 (26%) | Loss: 0.697505\n",
            "Train Epoch: 4 | Batch Status: 760/92650 (26%) | Loss: 0.692968\n",
            "Train Epoch: 4 | Batch Status: 780/92650 (27%) | Loss: 0.693035\n",
            "Train Epoch: 4 | Batch Status: 800/92650 (28%) | Loss: 0.693217\n",
            "Train Epoch: 4 | Batch Status: 820/92650 (28%) | Loss: 0.691276\n",
            "Train Epoch: 4 | Batch Status: 840/92650 (29%) | Loss: 0.694003\n",
            "Train Epoch: 4 | Batch Status: 860/92650 (30%) | Loss: 0.691308\n",
            "Train Epoch: 4 | Batch Status: 880/92650 (30%) | Loss: 0.688814\n",
            "Train Epoch: 4 | Batch Status: 900/92650 (31%) | Loss: 0.690622\n",
            "Train Epoch: 4 | Batch Status: 920/92650 (32%) | Loss: 0.692770\n",
            "Train Epoch: 4 | Batch Status: 940/92650 (32%) | Loss: 0.681676\n",
            "Train Epoch: 4 | Batch Status: 960/92650 (33%) | Loss: 0.690594\n",
            "Train Epoch: 4 | Batch Status: 980/92650 (34%) | Loss: 0.689100\n",
            "Train Epoch: 4 | Batch Status: 1000/92650 (35%) | Loss: 0.698762\n",
            "Train Epoch: 4 | Batch Status: 1020/92650 (35%) | Loss: 0.693419\n",
            "Train Epoch: 4 | Batch Status: 1040/92650 (36%) | Loss: 0.692322\n",
            "Train Epoch: 4 | Batch Status: 1060/92650 (37%) | Loss: 0.693783\n",
            "Train Epoch: 4 | Batch Status: 1080/92650 (37%) | Loss: 0.699977\n",
            "Train Epoch: 4 | Batch Status: 1100/92650 (38%) | Loss: 0.685128\n",
            "Train Epoch: 4 | Batch Status: 1120/92650 (39%) | Loss: 0.706969\n",
            "Train Epoch: 4 | Batch Status: 1140/92650 (39%) | Loss: 0.676678\n",
            "Train Epoch: 4 | Batch Status: 1160/92650 (40%) | Loss: 0.679852\n",
            "Train Epoch: 4 | Batch Status: 1180/92650 (41%) | Loss: 0.690178\n",
            "Train Epoch: 4 | Batch Status: 1200/92650 (41%) | Loss: 0.688858\n",
            "Train Epoch: 4 | Batch Status: 1220/92650 (42%) | Loss: 0.693737\n",
            "Train Epoch: 4 | Batch Status: 1240/92650 (43%) | Loss: 0.684159\n",
            "Train Epoch: 4 | Batch Status: 1260/92650 (44%) | Loss: 0.679375\n",
            "Train Epoch: 4 | Batch Status: 1280/92650 (44%) | Loss: 0.688783\n",
            "Train Epoch: 4 | Batch Status: 1300/92650 (45%) | Loss: 0.691408\n",
            "Train Epoch: 4 | Batch Status: 1320/92650 (46%) | Loss: 0.711725\n",
            "Train Epoch: 4 | Batch Status: 1340/92650 (46%) | Loss: 0.694019\n",
            "Train Epoch: 4 | Batch Status: 1360/92650 (47%) | Loss: 0.686403\n",
            "Train Epoch: 4 | Batch Status: 1380/92650 (48%) | Loss: 0.689063\n",
            "Train Epoch: 4 | Batch Status: 1400/92650 (48%) | Loss: 0.698847\n",
            "Train Epoch: 4 | Batch Status: 1420/92650 (49%) | Loss: 0.683675\n",
            "Train Epoch: 4 | Batch Status: 1440/92650 (50%) | Loss: 0.706039\n",
            "Train Epoch: 4 | Batch Status: 1460/92650 (50%) | Loss: 0.705569\n",
            "Train Epoch: 4 | Batch Status: 1480/92650 (51%) | Loss: 0.702205\n",
            "Train Epoch: 4 | Batch Status: 1500/92650 (52%) | Loss: 0.692718\n",
            "Train Epoch: 4 | Batch Status: 1520/92650 (52%) | Loss: 0.694575\n",
            "Train Epoch: 4 | Batch Status: 1540/92650 (53%) | Loss: 0.696811\n",
            "Train Epoch: 4 | Batch Status: 1560/92650 (54%) | Loss: 0.698616\n",
            "Train Epoch: 4 | Batch Status: 1580/92650 (55%) | Loss: 0.695188\n",
            "Train Epoch: 4 | Batch Status: 1600/92650 (55%) | Loss: 0.691389\n",
            "Train Epoch: 4 | Batch Status: 1620/92650 (56%) | Loss: 0.684185\n",
            "Train Epoch: 4 | Batch Status: 1640/92650 (57%) | Loss: 0.691212\n",
            "Train Epoch: 4 | Batch Status: 1660/92650 (57%) | Loss: 0.698619\n",
            "Train Epoch: 4 | Batch Status: 1680/92650 (58%) | Loss: 0.698996\n",
            "Train Epoch: 4 | Batch Status: 1700/92650 (59%) | Loss: 0.698355\n",
            "Train Epoch: 4 | Batch Status: 1720/92650 (59%) | Loss: 0.695128\n",
            "Train Epoch: 4 | Batch Status: 1740/92650 (60%) | Loss: 0.697975\n",
            "Train Epoch: 4 | Batch Status: 1760/92650 (61%) | Loss: 0.702048\n",
            "Train Epoch: 4 | Batch Status: 1780/92650 (61%) | Loss: 0.687590\n",
            "Train Epoch: 4 | Batch Status: 1800/92650 (62%) | Loss: 0.694687\n",
            "Train Epoch: 4 | Batch Status: 1820/92650 (63%) | Loss: 0.687056\n",
            "Train Epoch: 4 | Batch Status: 1840/92650 (64%) | Loss: 0.684063\n",
            "Train Epoch: 4 | Batch Status: 1860/92650 (64%) | Loss: 0.697779\n",
            "Train Epoch: 4 | Batch Status: 1880/92650 (65%) | Loss: 0.696945\n",
            "Train Epoch: 4 | Batch Status: 1900/92650 (66%) | Loss: 0.692643\n",
            "Train Epoch: 4 | Batch Status: 1920/92650 (66%) | Loss: 0.695282\n",
            "Train Epoch: 4 | Batch Status: 1940/92650 (67%) | Loss: 0.696072\n",
            "Train Epoch: 4 | Batch Status: 1960/92650 (68%) | Loss: 0.681713\n",
            "Train Epoch: 4 | Batch Status: 1980/92650 (68%) | Loss: 0.691546\n",
            "Train Epoch: 4 | Batch Status: 2000/92650 (69%) | Loss: 0.691213\n",
            "Train Epoch: 4 | Batch Status: 2020/92650 (70%) | Loss: 0.690544\n",
            "Train Epoch: 4 | Batch Status: 2040/92650 (70%) | Loss: 0.696663\n",
            "Train Epoch: 4 | Batch Status: 2060/92650 (71%) | Loss: 0.682831\n",
            "Train Epoch: 4 | Batch Status: 2080/92650 (72%) | Loss: 0.677568\n",
            "Train Epoch: 4 | Batch Status: 2100/92650 (73%) | Loss: 0.691220\n",
            "Train Epoch: 4 | Batch Status: 2120/92650 (73%) | Loss: 0.696375\n",
            "Train Epoch: 4 | Batch Status: 2140/92650 (74%) | Loss: 0.692194\n",
            "Train Epoch: 4 | Batch Status: 2160/92650 (75%) | Loss: 0.688242\n",
            "Train Epoch: 4 | Batch Status: 2180/92650 (75%) | Loss: 0.681774\n",
            "Train Epoch: 4 | Batch Status: 2200/92650 (76%) | Loss: 0.703977\n",
            "Train Epoch: 4 | Batch Status: 2220/92650 (77%) | Loss: 0.704536\n",
            "Train Epoch: 4 | Batch Status: 2240/92650 (77%) | Loss: 0.681108\n",
            "Train Epoch: 4 | Batch Status: 2260/92650 (78%) | Loss: 0.691194\n",
            "Train Epoch: 4 | Batch Status: 2280/92650 (79%) | Loss: 0.687199\n",
            "Train Epoch: 4 | Batch Status: 2300/92650 (79%) | Loss: 0.711086\n",
            "Train Epoch: 4 | Batch Status: 2320/92650 (80%) | Loss: 0.688782\n",
            "Train Epoch: 4 | Batch Status: 2340/92650 (81%) | Loss: 0.656678\n",
            "Train Epoch: 4 | Batch Status: 2360/92650 (81%) | Loss: 0.682097\n",
            "Train Epoch: 4 | Batch Status: 2380/92650 (82%) | Loss: 0.694389\n",
            "Train Epoch: 4 | Batch Status: 2400/92650 (83%) | Loss: 0.691202\n",
            "Train Epoch: 4 | Batch Status: 2420/92650 (84%) | Loss: 0.692683\n",
            "Train Epoch: 4 | Batch Status: 2440/92650 (84%) | Loss: 0.693432\n",
            "Train Epoch: 4 | Batch Status: 2460/92650 (85%) | Loss: 0.686697\n",
            "Train Epoch: 4 | Batch Status: 2480/92650 (86%) | Loss: 0.687480\n",
            "Train Epoch: 4 | Batch Status: 2500/92650 (86%) | Loss: 0.696278\n",
            "Train Epoch: 4 | Batch Status: 2520/92650 (87%) | Loss: 0.692811\n",
            "Train Epoch: 4 | Batch Status: 2540/92650 (88%) | Loss: 0.692662\n",
            "Train Epoch: 4 | Batch Status: 2560/92650 (88%) | Loss: 0.690235\n",
            "Train Epoch: 4 | Batch Status: 2580/92650 (89%) | Loss: 0.695337\n",
            "Train Epoch: 4 | Batch Status: 2600/92650 (90%) | Loss: 0.685858\n",
            "Train Epoch: 4 | Batch Status: 2620/92650 (90%) | Loss: 0.693231\n",
            "Train Epoch: 4 | Batch Status: 2640/92650 (91%) | Loss: 0.699694\n",
            "Train Epoch: 4 | Batch Status: 2660/92650 (92%) | Loss: 0.694739\n",
            "Train Epoch: 4 | Batch Status: 2680/92650 (93%) | Loss: 0.684365\n",
            "Train Epoch: 4 | Batch Status: 2700/92650 (93%) | Loss: 0.696700\n",
            "Train Epoch: 4 | Batch Status: 2720/92650 (94%) | Loss: 0.703832\n",
            "Train Epoch: 4 | Batch Status: 2740/92650 (95%) | Loss: 0.689323\n",
            "Train Epoch: 4 | Batch Status: 2760/92650 (95%) | Loss: 0.689126\n",
            "Train Epoch: 4 | Batch Status: 2780/92650 (96%) | Loss: 0.688999\n",
            "Train Epoch: 4 | Batch Status: 2800/92650 (97%) | Loss: 0.706396\n",
            "Train Epoch: 4 | Batch Status: 2820/92650 (97%) | Loss: 0.683630\n",
            "Train Epoch: 4 | Batch Status: 2840/92650 (98%) | Loss: 0.698249\n",
            "Train Epoch: 4 | Batch Status: 2860/92650 (99%) | Loss: 0.693498\n",
            "Train Epoch: 4 | Batch Status: 2880/92650 (99%) | Loss: 0.698513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpHvKvO4M8yr",
        "outputId": "bdcc482b-4dd5-4dcb-bcab-9ef3a8c7aa26"
      },
      "source": [
        "model.eval()\r\n",
        "test_loss = 0\r\n",
        "correct = 0\r\n",
        "\r\n",
        "for i, data in enumerate(test_loader):\r\n",
        "  inputs, labels = data\r\n",
        "  y_pred = model(inputs)\r\n",
        "\r\n",
        "  test_loss += criterion(y_pred, labels).item()\r\n",
        "\r\n",
        "  pred = y_pred.data.max(1,keepdim=True)[1]\r\n",
        "  #print(pred)\r\n",
        "  correct += pred.eq(labels.data.view_as(pred)).cpu().sum()\r\n",
        "\r\n",
        "test_loss /= len(test_loader.dataset)\r\n",
        "print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\r\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\r\n",
        "\r\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========================\n",
            "Test set: Average loss: 0.0108, Accuracy: 5502/10294 (53%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P73RriHTd6wo"
      },
      "source": [
        "##KNN\r\n",
        "\r\n",
        "def euclidean_distance(a, B):\r\n",
        "    \r\n",
        "    \r\n",
        "   \r\n",
        "    dist = []\r\n",
        "   \r\n",
        "    \r\n",
        "    for i in B:      \r\n",
        "        dist.append(np.sqrt(np.sum((i-a)**2)))\r\n",
        "        \r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    return dist\r\n",
        "\r\n",
        "def get_kneighbors(X_test, X_train, k, distance_metric=euclidean_distance):\r\n",
        "    \"\"\"\r\n",
        "    Description: Returns the neighbors of X_test in X_train\r\n",
        "    Input: X_test - test instances (list of instances or matrix)\r\n",
        "           X_train - training instances (list of instances or matrix)\r\n",
        "           k - k in kNN; #neighbors\r\n",
        "           distance_metric - distance metric\r\n",
        "    Output: index (array index or index in the input list) of the neighboring instances\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    \"\"\" ---- Beginning of code block (1) ---- \"\"\"\r\n",
        "    neighbors = []\r\n",
        "    pairwise_dist = [distance_metric(x_test, X_train) for x_test in X_test]    \r\n",
        "#     print(pairwise_dist)\r\n",
        "    \"\"\" ---- End of code block (1) ---- \"\"\"\r\n",
        "    \r\n",
        "    \r\n",
        "    \"\"\" ---- Beginning of code block (2) ---- \"\"\"\r\n",
        "    for dist in pairwise_dist:\r\n",
        "        dist_indexed = enumerate(dist)\r\n",
        "        dist_indexed_sorted = sorted(dist_indexed, key=lambda x: x[1])[:k]\r\n",
        "        \r\n",
        "        neighbors_dist = [instance[1] for instance in dist_indexed_sorted]\r\n",
        "        neighbors_index = [instance[0] for instance in dist_indexed_sorted]\r\n",
        "        \r\n",
        "        neighbors.append(neighbors_index)\r\n",
        "    \"\"\" ---- End of code block (2) ---- \"\"\"\r\n",
        "        \r\n",
        "    return np.array(neighbors)\r\n",
        "\r\n",
        "\r\n",
        "def knn_clf(X_test, X_train, y_train, k=3, distance_metric=euclidean_distance):\r\n",
        "    \"\"\"\r\n",
        "    Description: KNN Classification; classifies X_test according to (X_train, y_train)\r\n",
        "    Input: X_test - test instances (list of instances or matrix)\r\n",
        "           X_train - training instances (list of instances or matrix)\r\n",
        "           y_train - training labels\r\n",
        "           k - k in kNN; #neighbors\r\n",
        "           distance_metric - distance metric\r\n",
        "    Output: predictions\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    \"\"\" ---- Beginning of code block (3) ---- \"\"\"\r\n",
        "    ls_neighbors = get_kneighbors(X_test, X_train, k, distance_metric)\r\n",
        "#   print(ls_neighbors)\r\n",
        "#   print(type(ls_neighbors[1][0]))\r\n",
        "    y_pred = np.array([np.argmax(np.bincount(y_train[neighbors])) for neighbors in ls_neighbors])\r\n",
        "    \"\"\" ---- End of code block (3) ---- \"\"\"\r\n",
        "    \r\n",
        "    return y_pred \r\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYzt2gdGk92P",
        "outputId": "4e801dfd-a678-4a23-8e08-41ad3a606e34"
      },
      "source": [
        "\r\n",
        "data_tr = np.genfromtxt('dota2Train.csv', delimiter=',', skip_header=1)\r\n",
        "data_ts = np.genfromtxt('dota2Test.csv', delimiter=',', skip_header=1)\r\n",
        "\r\n",
        "#N_tr, d_tr = data_tr.shape\r\n",
        "#print(N_tr)\r\n",
        "#print(d_tr)\r\n",
        "#N_ts, d_ts = data_ts.shape\r\n",
        "#print(N_ts)\r\n",
        "#print(d_ts)\r\n",
        "\r\n",
        "X_tr = data_tr[:20000,1:]\r\n",
        "X_tr = np.where(X_tr ==-1,2,X_tr)\r\n",
        "\r\n",
        "Y_tr = data_tr[:20000,[0]].astype(np.int64).flatten()\r\n",
        "Y_tr = np.where(Y_tr==-1,0,Y_tr)\r\n",
        "\r\n",
        "X_ts = data_ts[:,1:]\r\n",
        "X_ts = np.where(X_ts ==-1,2,X_ts)\r\n",
        "\r\n",
        "Y_ts = data_ts[:,[0]].astype(np.int64).flatten()\r\n",
        "Y_ts = np.where(Y_ts==-1,0,Y_ts)\r\n",
        "print(X_tr.shape)\r\n",
        "print(Y_tr.shape)\r\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 116)\n",
            "(20000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IHbGTL6ljZO"
      },
      "source": [
        "def get_accuracy(Y_true, Y_pred):\r\n",
        "    \"\"\"\r\n",
        "    Measures the classification accuracy.\r\n",
        "    Input: Y_true - true label vector (ground truth)\r\n",
        "           Y_pred - predicted label vector\r\n",
        "    return a floating point number representing accuracy (a single number of accuracy rate)\r\n",
        "    \"\"\"\r\n",
        "    accuracy = Y_true == Y_pred\r\n",
        "    \r\n",
        "    \r\n",
        "    return accuracy.sum()/len(Y_true)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCmnradOt57f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "9d299a16-9fb4-411b-b7ad-c88c917ea64b"
      },
      "source": [
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline \r\n",
        "\r\n",
        "accuracies = []\r\n",
        "for num in range(3,41,2):\r\n",
        "    y_pred = knn_clf(X_ts,X_tr,Y_tr,k=num)\r\n",
        "    accuracies.append(get_accuracy(Y_ts,y_pred))\r\n",
        "\r\n",
        "plt.plot(range(3,41,2),accuracies)\r\n",
        "plt.xlabel(\"k\")\r\n",
        "plt.ylabel(\"Accuracy\")\r\n",
        "\r\n",
        "print(accuracies)    "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.512872826192558, 0.5093753036043913, 0.5055863208005441, 0.5037404061012338, 0.5028660254541921, 0.5037404061012338, 0.5068493150684932, 0.5104439910618868, 0.5118041387350627, 0.5087923831730302, 0.5134557466239191, 0.5117069853298358, 0.514232973865734, 0.5162731953754979, 0.5177304964539007, 0.5138443602448266, 0.5172447294277664, 0.5169532692120858, 0.5203536383950258]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEGCAYAAACtqQjWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV1bnw8d+TkTlhCGFIIIwCQUAIEcR5QLQKKg5QB3Co1Wq9fXtvr7V9r1bb3tvhvlqt3vZapKBVwaFVQIUioDJLGE0Yw5QBEkJCEiBkft4/zgaPmJATck72Ocnz/XzOx7OHtfZzjiRP1l5rryWqijHGGOMPYW4HYIwxpuWwpGKMMcZvLKkYY4zxG0sqxhhj/MaSijHGGL+JcDsAN3Xr1k2TkpLcDsMYY0LKxo0bj6pqXF3HWnVSSUpKIi0tze0wjDEmpIjIwfqO2e0vY4wxfmNJxRhjjN9YUjHGGOM3llSMMcb4jSUVY4wxfmNJxRhjjN9YUjHGGOM3llSMMaaV+cOnu/lyf1FA6rakYowxrUjmkeP84dM9rNtXGJD6A5pURGSSiOwSkUwR+Wkdx2eKSIGIbHFeDzn7R4nIWhHJEJFtInKXV5l+IrLeqXO+iEQ5+6Od7UzneFIgP5sxxoSi2asPEBURxt0X9wlI/QFLKiISDrwC3AAMA6aLyLA6Tp2vqqOc1yxnXxlwn6omA5OAP4hIrHPst8ALqjoQOAY86Ox/EDjm7H/BOc8YY4zj2MlK/r4ph9su6k3XDtEBuUYgWyqpQKaq7lPVSmAeMMWXgqq6W1X3OO8PAUeAOBER4GrgPefUucAtzvspzjbO8Wuc840xxgBvfZlFeVUt90/oF7BrBDKp9AayvbZznH1nm+rc4npPRBLPPigiqUAUsBfoChSranUddZ65nnO8xDn/7PoeFpE0EUkrKCg4v09mjDEhprK6ltfXHuCyQd24oEfHgF3H7Y76hUCSqo4AlvJ1SwMAEekJvAHcr6q1/rigqr6qqimqmhIXV+fMzcYY0+J8kn6Y/NIKHghgKwUCm1RyAe+WR4Kz7wxVLVTVCmdzFjDm9DER6QR8BPxcVdc5uwuBWBE5PWW/d51nruccj3HON8aYVk1VeW3VfvrHteeKwYH9YzqQSWUDMMgZrRUFTAMWeJ/gtEROmwzscPZHAf8AXlfV0/0nqKoCK4DbnV0zgA+d9wucbZzjy53zjTGmVUs7eIxtOSU8MKEfYWGB7WoO2CJdqlotIo8DS4BwYLaqZojIc0Caqi4AnhCRyUA1UATMdIrfCVwOdBWR0/tmquoW4Elgnoj8CtgMvOYcfw14Q0QynbqmBeqzGWNMKJm9aj8xbSO5bXRd3dr+Ja35j/mUlBS1lR+NMS1ZdlEZV/x+Bd+/YgBPThrilzpFZKOqptR1zO2OemOMMQE0Z80BwkS4b3zfZrmeJRVjjGmhjpdXMX9DNjde2JOeMW2b5ZqWVIwxpoV6Ny2HExXVPHBpYIcRe7OkYowxLVBNrTJnzQHG9O3MqMTYhgv4iSUVY4xpgT7dkU9WURkPNmMrBSypGGNMi/Taqv30jm3LxGHxzXpdSyrGGNPCpOeW8OX+ImZekkREePP+mrekYowxLczsVftpHxXOXanfmqM34CypGGNMC3KktJyF2w5xR0oindpENvv1LakYY0wL8sa6g1TXKvdPSHLl+pZUjDGmhSivquHN9VlcOzSevl3buxKDJRVjjGkh/rE5l6KTlQFfM+VcLKkYY0wLoKrMXrWfYT07Ma5/F9fisKRijDEtwMo9R9lz5AQPXtoPkcCumXIullSMMaYFeG3Vfrp1iOamkT0bPjmALKkYY0yIyzxynM93F3Df+L5ER4S7GoslFWOMCXGzVx8gKiKMuy/u43YollSMMSaUHTtZyd835XDrqN507RDtdjiWVIwxJpS99WUW5VW1zbpmyrkENKmIyCQR2SUimSLy0zqOzxSRAhHZ4rwe8jq2WESKRWTRWWVWep1/SEQ+cPZfKSIlXseeDuRnM8aEBlXl5eV7eOrvX7kdit9VVtfy+toDXDqwGxf06Oh2OABEBKpiEQkHXgGuA3KADSKyQFW3n3XqfFV9vI4qfg+0A77vvVNVL/O6xvvAh16HV6rqTf6I3xgT+lSVXy7awezV+wG4b3xfhvbs5HJU/vNJ+mHySyv4zW0j3A7ljEC2VFKBTFXdp6qVwDxgiq+FVXUZcLy+4yLSCbga+KCpgRpjWp7aWuXpDzOYvXo/d6UkEhUexvwN2W6H5Teqymur9tM/rj1XDI5zO5wzAplUegPe/wdznH1nmyoi20TkPRFpzDzNtwDLVLXUa994EdkqIp+ISHJdhUTkYRFJE5G0goKCRlzOGBMqamuVn/3jK95Yd5CHL+/Pb6ZeyMTkeD7YkktFdY3b4flF2sFjbMsp4f4J/QgLc+9hx7O53VG/EEhS1RHAUmBuI8pOB9722t4E9FXVkcAfqacFo6qvqmqKqqbExQVPdjfG+EdNrfKT97Yxb0M2j101gKduGIKIcGdKIsVlVSzdnu92iH4xe9V+YtpGMnV0XX+ruyeQSSUX8G55JDj7zlDVQlWtcDZnAWN8qVhEuuG5vfaRV12lqnrCef8xEOmcZ4xpJapravk/87fw/qYc/s+1g/m3iRecmbJkwsBu9I5tyztpOa7Fl3nkBJ/vLqDwREXDJ59DdlEZSzLy+O7FfWgXFbCu8fMSyGg2AINEpB+eZDIN+K73CSLSU1UPO5uTgR0+1n07sEhVy73q6gHkq6qKSCqehFnYxM9gjAkRVTW1PPH2Zj5Jz+PfJ13AD64c+I3j4WHC1DEJ/HH5HnKLT9E7tm2zx3fPrPXklXp+bfWMaUNyrxiG9+7E8F4xDO8dQ3ynaJ/m7Zqz5gBhItw3vm+gw260gCUVVa0WkceBJUA4MFtVM0TkOSBNVRcAT4jIZKAaKAJmni4vIiuBIUAHEckBHlTVJc7hacBvzrrk7cCjIlINnAKmqaoG6vMZY4JHRXUNj725mU935PN/vzOUhy7rX+d5d4xJ4KVle3h/Yw5PXDOoWWP8+KvD5JWW87MbhxAmQnpuCemHSlm2M5/Tv6m6to8iuXcMw3t1YnjvGIb3iiGxS9tvJJrj5VXM35DNjRf2pGdM8yZGX0hr/r2bkpKiaWlpbodhjGmC8qoaHvnbRj7bVcBzU5K5b3zSOc+/e9Y6DhaW8cVPrmq2Dm5V5ZZXVnO8vJpPf3zFN657sqKanXmlpOeWnkk0e/KPU13r+d3csU0Ew3vFkOwkmswjJ3h5RSYfPDaBUYmxzRL/2URko6qm1HUsuG7GGWNMI5yqrOF7r6exeu9R/uu2C5me2vDcV3emJPIv87awbl8hlwxsnm7XjQePsTWnhF9OSf5WImsfHcGYvl0Y0/frNVDKq2rYk3+C9EMlZxLN6+sOUlldC8CYvp1dSygNsaRijAlJJyuqeXDuBr7cX8Tvbx/J7WMSfCp3fXIPOrWJYH5adrMlldmrnZFaPsbYJjKcCxNiuDAh5sy+qppa9hacYPuhUsb07RyoUJvM7SHFxhjTaMfLq5gx+0s2HDjGC3eN8jmhgOcX9i0X9eaT9DxKyqoCGKVHdlEZi9PzmJ7atJFakeFhDOnRidtGJ7i2/rwvLKkYY0JKSVkV97z2JVuyi/nj9IuYMqrxz2ncmZJIZXUtC7bmNnxyE81dcwAJ0pFagWBJxRgTMo6drOTu19ax/VAJ/3P3aG688PxWORzeO4ZhPTsxPy2w07acqKg+M1KrVzMPYXaLJRVjTEg4eqKC6X9Zx+78E7x6XwoTk3s0qb67xiaSnltKxqESP0X4be+mZXO8opoHJiQF7BrBxpKKMSboHSktZ/qr6zhQeJLZM8Zy1QXdm1znlFG9iIoI490APWFfU6vMWXOA0X1iuahP8Has+5slFWNMUMsrKWfaq+vILT7FnPtTuXSQf0ZsxbaL4vrkHvxjcy7lVf6fZHLZjnwOFpbx4KV1P4jZUllSMcYEraqaWh59cyP5peW8/kAq4/p39Wv9d6YkUHIqMJNMvrZqP71j23J9crzf6w5mllSMMUHr+aW72ZxVzO9uH0lKUpeGCzTShAGnJ5n0b4d9em4J6/cXMeOSvkSEt65fs63r0xpjQsbKPQX86bO9TE9N5Dsjzm+UV0PCwoTbxySwKvMoOcfK/Fbv7NX7aRcVzl1jG37Cv6WxpGKMCToFxyv4P/O3Mqh7B56+qc719vzmjhTPg5PvbfRPh/2R4+Us3HqIO8YkENM20i91hhJLKsaYoFJbq/z4nS0cL6/i5e+Opm1UeECvl9C5HZcO7Ma7aTnU1jZ9gt2/rT1Ida0yc0I/P0QXeiypGGOCyqsr97Fyz1GevnkYF/To2CzXvCMlkdziU6zZ27QlmMqravjb+iyuGdKdft2CdyqVQLKkYowJGpuzjvHfS3Zx44U9+K4PMw77y8Rh8cS0jWxyh/2HW3IpOlnJA5e2zlYKWFIxxgSJklNV/PDtzcR3asN/3TbCpxUQ/aVNZDi3jOrF4ozzn2RSVXlt1X6G9uzEeD8PfQ4lllSMMa5TVX729684XFLOS9MvcqWD+86xnkkmPzzPSSZXZxayO/8ED0xIataEGGwsqRhjXDdvQzYffXWYf5042LW1QpKd1RXnbzi/W2CvrdpHtw5RTB7Vy8+RhRZLKsYYV+3OP86zCzO4dGA3Hrl8gKux3DU2kYxDnmV9GyPzyAlW7CrgnnF9iY4I7Gi1YGdJxRjjmvKqGh5/axMdoiN4/q6RzbZmfH2mjOztTDLZuNbKnDX7iYoI455xrWPNlHMJaFIRkUkisktEMkXkp3UcnykiBSKyxXk95HVssYgUi8iis8rMEZH9XmVGOftFRF5yrrVNREYH8rMZY5ruuUXb2Z1/gufvHEX3jm3cDoeYdpFMSu7BB1sO+TzJZHFZJe9vzOWWUb3o1iE6wBEGv4AlFREJB14BbgCGAdNFZFgdp85X1VHOa5bX/t8D99ZT/U+8ymxx9t0ADHJeDwN/8ssHMcYExMdfHeat9Vl8/4r+XD44zu1wzrgzJZGSU1X808dJJt/+MptTVTWtehixt0C2VFKBTFXdp6qVwDxgiq+FVXUZcLwR15sCvK4e64BYEQnMhEHGmCbJLirjyfe3MSoxln+beIHb4XzDJQO6ktC5Le/40GFfVVPL3DUHmDCwK0N6dGqG6IJfIJNKb8D7/0qOs+9sU53bVe+JSKKPdf/aKfOCiJxub/p0PRF5WETSRCStoKDAx8sZY/ylqqaWJ+ZtBoU/Tr+IyCCbxTcsTLhjTCKr9x4lu+jck0x+/NVh8krLeaCVTslSF7f/by4EklR1BLAUmOtDmaeAIcBYoAvwZGMuqKqvqmqKqqbExQVPk9uY1uIFZzr7/7ztQhK7tHM7nDrd7sMkk6rK7FX76d+tvV9WomwpAplUcgHvlkeCs+8MVS1U1QpncxYwpqFKVfWwc4urAvgrnttsPl3PGOOulXsK+NPnnunsbx4ZvM9z9I5ty6UDu/HexvonmdyUdYytOSXcPyHJ9VFrwSSQSWUDMEhE+olIFDANWOB9wll9HpOBHQ1VerqMeB5ZvQVIdw4tAO5zRoGNA0pU9XDTP4Yxxh9OT2c/MC7w09n7w53OJJOr9x6t8/hrq/bTqU0Et41OaObIgltEoCpW1WoReRxYAoQDs1U1Q0SeA9JUdQHwhIhMBqqBImDm6fIishLPba4OIpIDPKiqS4A3RSQOEGAL8IhT5GPgRiATKAPuD9RnM8Y0jvd09n97KDXg09n7w8TkeGLbRfJOWg6XDfrmrfKcY2UsTs/je5f3p310wH6NhqSAfhuq+jGeX/be+572ev8Unj6SuspeVs/+q+vZr8Bj5x2sMSZg/uJMZ//rW4eHzCip6IhwbhnVm7e+zKK4rJLYdlFnjs1dcwARYcb4JPcCDFJud9QbY1q4zVnH+P2SXdwwvHmns/eHO1M8k0x+sPnr7tkTFdXM25DNDcN70Cu2rYvRBSdLKsaYgCkt/3o6+98083T2/jCsVyeG9+7EO2lfjwJ7Ly2b4+XVPGgPO9bJkooxJmB+t3jn19PZtwvN9drvSklk+2HPJJM1tcpf1xzgoj6xXNTHndmUg50lFWNMQKTnlvDm+izuG9/Xtens/WGyM8nkO2nZLN95hIOFZdZKOQcbtmCM8TtV5ZkFGXRpF8WPrh3sdjhNEtMukhuG9+CDzblkHCqlV0wbJiX3cDusoGUtFWOM3/1jcy4bDx7jyRuGuLKKo7/dlZJIaXk1Gw8eY8YlSUQE2dQywcS+GWOMXx0vr+I/P97JqMRYbm8hDwaO69+VxC5taRcVzrSxoTWCrbnZ7S9jjF+9tGwPhScrmD0zpcVMXxIWJvxu6kiOl1eF7ICD5mJJxRjjN3vyj/PX1QeYNjaREQmxbofjV+MHdHU7hJBgt7+MMX6hqvxiYQbtosKDbo0U03wsqRjTAlVW1/L2l1kcL69qtmsuTs9jdWYh/3b9BXS1ZXVbLUsqxrRAC7Ye4qm/f8UP3txEVU1twK93qrKGX320g6E9O4XcVCzGvxpMKiJys4hY8jEmhCxOz6NtZDgr9xzlmQUZeOZbDZw/fZZJbvEpnp2cbMNtWzlf/u/fBewRkd+JyJBAB2SMaZqTFdV8saeAaamJPHrlAN5an8WslfsDdr2DhSf58xf7uGVUL1L7dQnYdUxoaHD0l6reIyKdgOnAHBFRPCsuvq2qxwMdoDGmcT7bVUBldS2TknswNqkLWYVl/OcnO+jTtR3XB+BJ8F8u2k5kmPDUjUP9XrcJPT61U1W1FHgPmAf0BG4FNonIDwMYmzHmPCzOyKNbhyhSkroQFib8vztHMjIhln+Zt5ltOcV+vdaKnUf4dMcRnrhmEPGd2vi1bhOafOlTmSwi/wA+AyKBVFW9ARgJ/GtgwwteJyuq3Q7BmG8pr6ph+Y58rhvWg3DnwcM2keH85b4UunWI5sG5aeQWn/LLtSqqa3h2YQb949pz/wSbYNF4+NJSmQq8oKoXqurvVfUIgKqWAQ8GNLog9dG2w6T86lO//XAa4y+rM49ysrKGScO/eZsrrmM0f505lvLKGh6cs8EvQ41fW7WfA4Vl/OLmZKIirHPeePjyL+EXwJenN0SkrYgkAajqsoBEFeRG9YmlorqGN9YedDsUY75hcXoeHdtEML7/t5/+HhTfkf+5ZzR7jpzg8bc2U92EocaHS07xx2WZXJ8cz+WD4xouYFoNX5LKu4D3v74aZ1+DRGSSiOwSkUwR+Wkdx2eKSIGIbHFeD3kdWywixSKy6Kwybzp1povIbBGJdPZfKSIlXnU97UuM56N3bFsmDuvBvA1ZlFfVBOoyxjRKdU0tS3fkc+3Q+HpbDpcNiuNXtwzn890FPLtw+3kPNf71RzuoVeX/fmdYU0I2LZAvSSVCVStPbzjvoxoqJCLhwCvADcAwYLqI1PUvcL6qjnJes7z2/x64t47z3wSGABcCbYGHvI6t9KrruYZibIqZE5IoLqviwy25DZ9sTDP4cn8RxWVVDY7wmp7ah+9f3p831h1k9uoDjb7O2r2FLNp2mEevHEBil3bnGa1pqXxJKgUiMvn0hohMAY76UC4VyFTVfU4imgdM8TUw59bat4Ysq+rH6sBzW86VubUv7teFIT06MmfNwYA/WGaMLxZn5NEmMowrfLgd9eSkIUxK7sGvPtrO0u35Pl+juqaWXyzIIKFzWx65YkBTwjUtlC9J5RHgZyKSJSLZwJPA930o1xvI9trOcfadbaqIbBOR90Qk0Yd6AXBue90LLPbaPV5EtorIJyKSXE+5h0UkTUTSCgoKfL1cXfUw85Ikdhwu5cv9ReddjzH+UFurLMnI48rB3WkbFd7g+WFhwgt3jWJE7xieeHsz6bklPl3njXUH2ZV/nP+4aRhtIhu+jml9GkwqqrpXVcfhuYU1VFUvUdVMP11/IZCkqiOApcDcRpT9H+ALVV3pbG8C+qrqSOCPwAd1FVLVV1U1RVVT4uKa1sE4ZVRvYtpGMnftgSbVY0xTbc4uJr+0ghsu9P3hxrZR4fxlRgpd2kfx4NwNHC4592jGguMVPP/P3Vw+OI6Jw+KbGrJpoXwaBygi3wF+APxYRJ72sRM8F/BueSQ4+85Q1UJVrXA2ZwFjfIznGSAO+LFXXaWqesJ5/zEQKSLdfKnvfLWNCmfa2ESWZORzyIYXGxctycgjMly4akj3RpXr3rENs2eO5WRFDQ/MSePEOZ6/+t3inZRX1/DMzcMQaRmLbxn/8+Xhxz/jmf/rh4AAdwB9fah7AzBIRPqJSBQwDVhwVt09vTYnAzt8iOch4HpguqrWeu3vIc6/dBFJxfPZCn2Is0nuGdcXVeVv62x4sXGHqrI4PY8JA7vRqU3jVyW8oEdHXrl7NLvzj/PE23UPNd6cdYx3N+bwwKX9GBDXwR9hmxbKl5bKJap6H3BMVZ8FxgODGyqkqtXA48ASPMniHVXNEJHnvDr+nxCRDBHZCjwBzDxdXkRW4hm6fI2I5IjI9c6hPwPxwNqzhg7fDqQ7db0ETNNm6EFP7NKOa4fG8/aXNrzYuGPH4eNkFZUxqQnzel0xOI5nJyezfOcRfvXRN/+2q61VnlmQQXynaH549aCmhmtaOF+WEy53/lsmIr3w/PXf8xznn+Hchvr4rH1Pe71/CniqnrKX1bO/zphV9WXgZV/i8reZE5L45/Z8Fmw9xJ0pPo81MMYvFmfkESZwbRP7Oe4Z15cDR08ya9V+krq2Y6Yz9co7adlsyynhxWmj6BBtK5Cbc/OlpbJQRGLxPDeyCTgAvBXIoELN+P5duSC+I3PXHLDhxabZLUnPY2xSF7r5YbXFp24cynXD4nlu0XaW78ynuKyS3y7eSWq/Lkwe2csP0ZqW7pxJxVmca5mqFqvq+3j6UoZ4tzaMZ3jxjEuSyDhUStrBY26HY1x04OhJ7p61jlV7fHmUq+n2FZxgV/7xb831db7Cw4QXp40iuVcMj7+1mX99Zyslp6p4dnKydc4bn5wzqTgd4a94bVeoqm8D2luZWy7qRac2EcxZc8DtUIxL0nNLmPqnNazOLOS5RRnU1ga+1bokw/Pgoj/XSWkXFcGsGSnEtI1k2c4j3DuuL0N7dvJb/aZl8+X21zIRmSr2Z8o5tYuKYFpqHxan5zU43t+0PKszj3LX/66lTWQ4P75uMLvzT/BJel7Ar7s4/TAjE2PpFdvWr/XGd2rDnPtTmZ7ahx9fd4Ff6zYtmy9J5ft4RmFViEipiBwXkdIAxxWS7h3Xl1pV3lyX5XYophl9tO0w9/91Awmd2/H+o5fw2FUDGRDXnpeW7QloayW3+BRbc0qaNOrrXC7o0ZH/uu1CYto1fpiyab18eaK+o6qGqWqUqnZytq0tXIfTw4vfsuHFrcYbaw/w+NubGJkYwzvfH0+PmDaEhwlPXDOIXfnHWZIRuNbKP526r0+2p9tN8PDl4cfL63o1R3ChaOYlSRSdrGTRtsNuh2ICSFV5fulu/uPDDK4ZEs8bD178jb/obxrRi/5x7XkxgK2Vxel5XBDfkf72MKIJIr7c/vqJ1+s/8MzX9YsAxhTSLhnQlUHdOzBnzX4bXtxC1dQqP/8gnZeW7eHOlAT+fM/ob02uGB4m/PDqgezMO84/GzELsK+Onqhgw4EirvfTqC9j/MWX2183e72uA4YDNm62HqeHF6fnlrIpy76mlqa8qobH3tzEW+uz+MGVA/jt1BFEhNf9Y3TziF706xaY1sqn2/OpVQLWn2LM+TqfhaVzgKH+DqQluW10bzq2ieCv57EAkglepeVVzJj9JYsz8nj6pmH8+6Qh53x2IyI8jMevGsiOw6Us3eHf1srijDz6dGnH0J4d/VqvMU3lS5/KH0XkJef1MrASz5P1ph7toiK4KyWRxel55JWUN1zABL0jpeXc9b/r2HjwGC9OG8UDl/bzqdyUUb1I6tqOl5bt8dvt0JJTVazOPMqk4T3sgUQTdHxpqaQBG53XWuBJVb0noFG1APeNT6JGlTfX2+zFoe7A0ZNM/fMaDhaeZPbMsUwZVddac3WLCA/jsasGknGolE93HPFLPCt2HqGqRv32FL0x/uRLUnkP+JuqzlXVN4F1ImILUzegT9d2XDOkO2+tz6Ki2oYXh6r03BJu//MaTpRX89b3xnG5D0v1nu3Wi3rTt2s7Xly22y+tlcXpecR3imZUQmyT6zLG33x6oh7wfly3LfBpYMJpWWZe0o/Ck5Us2mrDi0PRmsyjTHt1HdER4bz36CWMSjy/X+KnWyvpuaUs39m01sqpyho+232E65N7EBZmt75M8PElqbQ5vaIigPPeWio+mDCwKwO7d2COzV4ccj7adpiZf91A79i2vP/oJU1emOrWi3qT2KUtLzaxb+Xz3QWUV9XaqC8TtHxJKidFZPTpDREZA9jkVj44Pbz4q9wSNmUVux2O8VFdT8k3VaQzEmxbTgkrdp1/a2VJRh6x7SJJ7delyTEZEwi+JJUfAe+KyEoRWQXMx7Oio/HBbRd5hhfPtdmLQ8JLy/bU+5R8U902OoGEzm158dPza61UVtfy6Y58rhsaX++zMca4zZeHHzcAQ4BHgUeAoaq6MdCBtRTtoyO4MyWRj786TH6pDS8OZgeOnuT5pbuZMqpXnU/JN1Wk07eyNaeEz3YXNLr82n2FHC+vtlFfJqj58pzKY0B7VU1X1XSgg4j8IPChtRz3je/rDC+22YuD2d/WHSQiTPj5jUMD1hKYOjqB3rHn11pZnH6Y9lHhTBjYLSCxGeMPvvzkfE9Vz3QIqOox4Hu+VC4ik0Rkl4hkishP6zg+U0QKRGSL83rI69hiESkWkUVnleknIuudOueLSJSzP9rZznSOJ/kSY3Po27U9V1/QnbfWH7ThxUHqVGUN727M4frhPejeqel9KPWJivC0VrZkF/NFI1aHrKlV/pmRz9VD4/3egjLGn3xJKuHeC3SJSDgQ1VAh57xXgBuAYcB0ERlWx6nzVXWU85rltf/3wL11nP9b4AVVHYhnDrIHnf0PAsec/S845wWNGZckcfREJR9/ZcOLg9HCrYcoOVXFfeP6Bvxat4853cmHPSkAABm1SURBVFrx/bmVtANFFJ6stFFfJuj5klQWA/NF5BoRuQZ4G/jEh3KpQKaq7lPVSmAeMMXXwFR1GXDce5+T3K7G80AmwFzgFuf9FGcb5/g1wbRa5WWDujEgrj1zbD6woKOqvL7uAIPjOzTLqKqoiDAevXIAm7KKWZXpW2tlcUYeURFhXHlB4x++NKY5+ZJUngSW4+mkfwT4im8+DFmf3kC213aOs+9sU0Vkm4i8JyKJDdTZFShW1eo66jxzPed4iXP+N4jIwyKSJiJpBQWN7yw9X6eHF2/NKWGzzV4cVLZkF5OeW8q945OabS6tO1IS6BnThj/40LeiqixJz+PyQXG0j45olviMOV++jP6qBdYDB/C0Pq4Gdvjp+guBJFUdASzl65ZGwKjqq6qaoqopcXHN+1ffbaMT6BgdwRwbXhxU3lh7kA7REdx6ke9zejVVdEQ4P7hyABsPHmN1ZuE5z/0qt4RDJeU26suEhHqTiogMFpFnRGQn8EcgC0BVr1LVl32oOxfwbnkkOPvOUNVCVa1wNmcBYxqosxCIFZHTf65513nmes7xGOf8oNEhOoLbUxL4+KvDHLHhxUHh9Cqdt43uTYdmbgXcOTaRHp3aNDgn2OL0PMLDhGuHdm/G6Iw5P+dqqezE0yq5SVUvVdU/Ao0ZurQBGOSM1ooCpgELvE8QkZ5em5NpoAWknp+8FcDtzq4ZwIfO+wXONs7x5RqEc6PMGJ9Eda0NLw4W8zdkU1lTyz3N0EF/tuiIcH5w1QA2HDjG2r11//2jqixOz2N8/67EtmtwfIwxrjtXUrkNOAysEJG/OJ30Pt9wdvo1HgeW4EkW76hqhog8JyKTndOeEJEMEdkKPAHMPF1eRFYC7+LpcM8RkeudQ08CPxaRTDx9Jq85+18Dujr7fwx8awhzMEjq1p4rB8fx5vosKqtr3Q6nVaup9SxNMK5/FwbHu7PY1Z0picR3iuYPy/bUeXzPkRPsO3rSlg02IaPepKKqH6jqNDxP06/AM11LdxH5k4hM9KVyVf1YVQer6gBV/bWz72lVXeC8f0pVk1V1pHNbbadX2ctUNU5V26pqgqoucfbvU9VUVR2oqnecvn2mquXO9kDn+L7z/VICbeaEfhw9UWHDi1322a4j5Bw7xb3jklyLoU1kOI9eMYAv9xfV2VpZnJ6HCFw/LN6F6IxpPF866k+q6luqejOePozNeFoL5jxdNrAb/bu1tw57l72x7iDdO0YzMdndX9jTUvvQvWM0f/h097eOLU7PY0yfzgF9INMYf2rUXBSqeswZPXVNoAJqDcLChPvG93WGspa4HU6rdLDwJJ/vLmB6ah8iXZ6csU1kOI9cMYD1+4tYt+/r1kpWYRnbD5faqC8TUmyqU5fcelEC0RFhvP2lddi74c31WYSJ8N2L+7gdCgDfvbgPcR2jefHTr/tWlmTkAXC9PUVvQoglFZfEtIvkOyN68uGWQ5RVVjdcwPhNeVUN76Rlc31yPPFBclvpdGtl7b5CvtxfBHieok/u1YnELrYmngkdllRcND21Dycqqlm0zTrsm9PCrYcoLqtytYO+Lndf3IduHaJ5cdlujpSWs/HgMZvry4QcSyouSunbmYHdOzDPboE1qzfWHWRQ9w6M6x9cqyd6Wiv9WZ1ZyH9+7Hlky/pTTKixpOIiEWHa2EQ2ZRWzO/94wwVMk23NLmZbTgn3ju/bbPN8NcbdF/elW4coPthyiP5x7RnYvYPbIRnTKJZUXHbb6ASiwq3Dvrm8vvYg7aPCm3Wer8ZoGxXOw5f3B+CG4T2CMvEZcy6WVFzWpX0UE5Pj+fumXMqrbAGvQDp2spKF2w5x6+jedGzjv7Xn/e3ecUnMvCSJuy9u/qljjGkqSypBYHpqH0pOVbE4Pc/tUFq0d9KyqayuDboO+rO1jQrnF5OT6RXrywoTxgQXSypBYHz/rvTp0s5ugQVQTa3yt/UHSe3XhQt6uDPPlzGtgSWVIBAWJtw1NpH1+4vYV3DC7XBapC92F5BddIp7XZiN2JjWxJJKkLhjTALhYcL8DdkNn2wa7fW1B4jrGG1PpxsTYJZUgkT3Tm24Zkh33tuYY1Pi+1lWYRmf7S5g+thEoiLsn7wxgWQ/YUFkemofCk9W8umOfLdDaVHeXH/QmefLbn0ZE2iWVILI5YPj6BXTxjrs/ai8qob5adlMHBZPj5jgmOfLmJbMkkoQCQ8T7hybyKrMo2QXlbkdTouwaNthZ54va6UY0xwsqQSZO1MSETzPVJime2PdQQbEtWf8gK5uh2JMq2BJJcj0im3LFYPjeCctm+oa67Bvim05xWzNLubeccE5z5cxLVFAk4qITBKRXSKSKSI/reP4TBEpEJEtzushr2MzRGSP85rh7Ovode4WETkqIn9oqK5QMy21D/mlFXy2q8DtUELaG2sP0i4qnNvGJLgdijGtRkSgKhaRcOAV4DogB9ggIgtUdftZp85X1cfPKtsFeAZIARTY6JQ9BozyOm8j8Pdz1RWKrh7SnbiO0czbkMW1w9xdPz1UHTtZyYKth5g6JoFOQTzPlzEtTSBbKqlApqruU9VKYB4wxcey1wNLVbXISSRLgUneJ4jIYKA7sNKPMQeFyPAw7hiTwPKdR8grKXc7nJD03sYcKqprrYPemGYWyKTSG/Dubc5x9p1tqohsE5H3RCSxEWWn4WmZaAN1haS7xiZSq9Zhfz5qnXm+xiZ1ZmjPTm6HY0yr4nZH/UIgSVVH4GmNzG1E2WnA242tS0QeFpE0EUkrKAjePou+XdszYWBX5m/IprZWGy5gzvhiTwEHC8u4d3yS26EY0+oEMqnkAt6thQRn3xmqWqiqFc7mLGCML2VFZCQQoaobfajrG1T1VVVNUdWUuLi4xn+qZjRtbB9yi0+xMvOo26GElDfWHqRbh2hb390YFwQyqWwABolIPxGJwtOyWOB9goj09NqcDOxw3i8BJopIZxHpDEx09p02nW+2Us5VV8iamBxP53aRtoZ9I2QXlbF81xGmp9o8X8a4IWCjv1S1WkQex5MMwoHZqpohIs8Baaq6AHhCRCYD1UARMNMpWyQiv8STmACeU9Uir+rvBG4865J11hXKoiPCmTo6gTlrDlBwvIK4jtFuhxT03lyfheCZR80Y0/zkm/3crUtKSoqmpaW5HcY5ZR45wbXPf85PbxjCI1cMcDucoFZeVcP4/1pGar8u/O+9KW6HY0yLJSIbVbXOHzK7PxDkBnbvQGpSF+ZvyKY1/wHgi4+/OsyxsqqgXy7YmJbMkkoImJaayP6jJ1m3r6jhk1spVWXu2oP0j/OMmjPGuMOSSgi48cKedGoTwbwN1mFfn7V7C9maXcz9lyTZPF/GuMiSSghoExnOrRf15pP0PIrLKt0OJyi9vCKTuI7R3JES0s+8GhPyLKmEiGmpfaisruXvm3IbPrmV2ZR1jDV7C/neZf1oExnudjjGtGqWVELE0J6dGJkYy7wNWdZhf5ZXlmcS2y6Su225YGNcZ0klhEwfm8ju/BNsyip2O5Sgsf1QKct2HuGBCf1oHx2wx66MMT6ypBJCbh7Zi/ZR4baGvZdXPsukQ3QEM2yeL2OCgiWVENI+OoLJo3qxaNshSsur3A7HdZlHTvDxV4e5d3xfYtrZminGBANLKiFm2tg+lFfV8uGWQ26H4ro/fbaX6IgwHry0n9uhGGMcllRCzIiEGIb27NTqJ5nMLirjgy25TE/tQ7cONieaMcHCkkqIERG+m5pIxqFSvsopcTsc1/zvF3sJE3j48v5uh2KM8WJJJQRNuag3bSLDeLuVPmGfX1rOO2k53D4mgZ4xbd0OxxjjxZJKCOrUJpLvXNiLBVsOcbKi2u1wmt1fvthHdU2tzdpsTBCypBKipqcmcqKimo+2HXY7lGZVdLKSN9dnMXlkL/p2be92OMaYs1hSCVFj+nZmYPcOvPll63rC/q+r93OqqoYfXDXQ7VCMMXWwpBKiRIQHJvRja3Yxf/p8r9vhNIvS8irmrDnApOQeDI7v6HY4xpg6WFIJYdNTE7l5ZC9+v2QXy3fmux1OwL2x9iDHy6t5zFopxgQtSyohTET43dQRDOvZiX95ewt7C064HVLAlFVW89qq/VwxOI4LE2LcDscYUw9LKiGubVQ4r96XQlREGN97Pa3FTt/y9pfZFJ2s5PGrrZViTDALaFIRkUkisktEMkXkp3UcnykiBSKyxXk95HVshojscV4zvPZ/5tR5ukx3Z3+0iMx3rrVeRJIC+dmCSe/YtvzP3aPJKizjR/O2UFPbsjruK6prePWLvaT268LYpC5uh2OMOYeAJRURCQdeAW4AhgHTRWRYHafOV9VRzmuWU7YL8AxwMZAKPCMinb3K3O1V5oiz70HgmKoOBF4AfhuYTxacLu7flWcmJ7N85xGeX7rL7XD86v2NueSXVvBDa6UYE/QC2VJJBTJVdZ+qVgLzgCk+lr0eWKqqRap6DFgKTGqgzBRgrvP+PeAaaWWLld9zcR+mpybyyoq9LNrWMiacrK6p5c+f72VkQgyXDuzmdjjGmAYEMqn0BrK9tnOcfWebKiLbROQ9ETm9wHhDZf/q3Pr6D6/EcaaMqlYDJUDXsy8mIg+LSJqIpBUUFJzXBwtWIsKzk4eT0rczP3l3GxmHQn9usIXbDpFVVMZjVw2klf2NYExIcrujfiGQpKoj8LRG5jZwPnhufV0IXOa87m3MBVX1VVVNUdWUuLi4Rgcc7KIiwvife0YT0zaSh1/fSOGJCrdDOm+1tcorK/ZyQXxHrh0a73Y4xhgfBDKp5AKJXtsJzr4zVLVQVU//1psFjGmorKqe/u9x4C08t9m+UUZEIoAYoNBPnyWkdO/YhlfvG8PRExU89tYmqmpq3Q7pvCzJyCPzyAl+cNUAwsKslWJMKAhkUtkADBKRfiISBUwDFnifICI9vTYnAzuc90uAiSLS2emgnwgsEZEIEenmlI0EbgLSnTILgNOjxG4Hlmtrmr/kLCMSYvnN1AtZt6+IXy3a7nY4jaaqvLwik6Su7bhpRC+3wzHG+CgiUBWrarWIPI4nQYQDs1U1Q0SeA9JUdQHwhIhMBqqBImCmU7ZIRH6JJzEBPOfsa48nuUQ6dX4K/MU55zXgDRHJdOqaFqjPFipuvSiBjNxSZq3az7BenbhrbB+3Q/LZZ7sLyDhUyu+mjiDcWinGhAxpxX/Mk5KSomlpaW6HEVDVNbXcP2cD6/YVMu/h8Yzp27nhQi5TVW7/81oOF5/is59cRVSE211/xhhvIrJRVVPqOmY/rS1cRHgYf5x+ET1j2vLI3zaSV1LudkgNWreviI0Hj/H9KwZYQjEmxNhPbCsQ2y6KWTNSKKuo5vtvpFFeVeN2SOf0yopMunWI5q6xiQ2fbIwJKpZUWonB8R15/q5RbM0p4ef/SA/aNVi2ZBezKvMoD13WjzaR4W6HY4xpJEsqrcj1yT340bWDeH9TDrNXH3A7nDq9vDyTmLaR3DOur9uhGGPOgyWVVuaJqwcxcVg8//nxDlbtOep2ON+w43Apn+7I5/4JSXSIDtjARGNMAFlSaWXCwoTn7xrFgLj2PP72JrIKy9wO6YxXVmTSPiqcmZckuR2KMeY8WVJphTpER/CX+1JQhe+9nsbJimq3Q2JfwQk++uow94zvS2y7KLfDMcacJ0sqrVTfru15+bsXsefIcR59cxNHSt0balxSVsUvF20nKjyMhy7t71ocxpims6TSil02KI5f3jKcdXsLueq/P+PPn++lorr5hhvX1Cpvrc/iyv9ewee7C/jXiYOJ6xjdbNc3xvifPVHfwp+o98WBoyf51Uc7+HRHPkld2/H0zcO4ekhgZwXecKCIXyzIIONQKan9uvCLm5MZ1qtTQK9pjPGPcz1Rb0nFksoZn+06wnOLtrOv4CRXXRDHf9w0jP5xHfx6jcMlp/ivj3eyYOshesa04Wc3DuWmET1trRRjQogllXpYUvm2yupaXl97gD98uoeK6hoemNCPx68eSMc2kU2qt7yqhtdW7efl5ZnUqPLI5f155MoBtIuyocPGhBpLKvWwpFK/guMV/H7JTt5JyyGuYzRPThrCbRf1bvS6JqrK0u35/OqjHWQVlTEpuQc//85QEru0C1DkxphAs6RSD0sqDduSXcwvFmSwJbuYUYmxPDs5mZGJsT6VzTxynGcXbmflnqMM6t6BZ25O5tJBts68MaHOkko9LKn4prZW+cfmXH6zeCcFxyu4Y0wC/z5pSL0jtUrLq3jx0z3MXXOAtlHh/Pi6wdwzri+R4TbY0JiW4FxJxW5omwaFhQlTxyQwMTmel5dnMnv1fhan5/HENYOYcUnSmenpa2uVdzdm87vFuygqq2Ta2D7828TBdO1gw4SNaS2spWItlUbbV3CCXy7azopdBfSPa8/TNw2jY5tInl2YwbacEsb07cyzk5MZ3jvG7VCNMQFgt7/qYUmlaZbvzOeXi3aw/+hJAOI7RfOzG4cyeWQvGyJsTAtmt79MQFw9JJ4JA7vx1vosTlXVMGN8Eu1tdmFjWrWA9pyKyCQR2SUimSLy0zqOzxSRAhHZ4rwe8jo2Q0T2OK8Zzr52IvKRiOwUkQwR+Y0vdZnAiY4I5/4J/fjBlQMtoRhjAtdSEZFw4BXgOiAH2CAiC1R1+1mnzlfVx88q2wV4BkgBFNgoIguACuC/VXWFiEQBy0TkBlX9pL66jDHGNJ9AtlRSgUxV3aeqlcA8YIqPZa8HlqpqkaoeA5YCk1S1TFVXADh1bgISAhC7McaY8xDIpNIbyPbaznH2nW2qiGwTkfdEJNHXsiISC9wMLGugLs4q97CIpIlIWkFBQSM/kjHGmHNx+2m0hUCSqo7A0xqZ60shEYkA3gZeUtV9jalLVV9V1RRVTYmLi2vyBzDGGPO1QCaVXMC7tZDg7DtDVQtVtcLZnAWM8bHsq8AeVf2DD3UZY4xpJoFMKhuAQSLSz+lUnwYs8D5BRHp6bU4GdjjvlwATRaSziHQGJjr7EJFfATHAj3ysyxhjTDMJ2OgvVa0WkcfxJINwYLaqZojIc0Caqi4AnhCRyUA1UATMdMoWicgv8SQmgOecfQnAz4GdwCbnAbuXVXVWfXUZY4xpPvZEvT1Rb4wxjWLTtNRDRAqAgy6G0A046uL1GyNUYrU4/StU4oTQibUlxNlXVesc6dSqk4rbRCStvmwfbEIlVovTv0IlTgidWFt6nG4PKTbGGNOCWFIxxhjjN5ZU3PWq2wE0QqjEanH6V6jECaETa4uO0/pUjDHG+I21VIwxxviNJRVjjDF+Y0nFJSJyQES+chYUC5onMEVktogcEZF0r31dRGSps2DaUmfqHNfVE+svRCTXa7G2G92M0YkpUURWiMh2Z3G5f3H2B9X3eo44g+o7FZE2IvKliGx14nzW2d9PRNY7iwLOd6aHCsY454jIfq/vc5SbcZ4mIuEisllEFjnb5/V9WlJx11WqOirIxqzPASadte+nwDJVHYRnqYFvreLpkjl8O1aAF5zvdZSqftzMMdWlGvhXVR0GjAMeE5FhBN/3Wl+cEFzfaQVwtaqOBEYBk0RkHPBbPHEOBI4BD7oYI9QfJ8BPvL7PLe6F+A3/wjfnTDyv79OSivkGVf0Cz9xp3qbw9VICc4FbmjWoetQTa9BR1cOqusl5fxzPD25vgux7PUecQUU9Tjibkc5LgauB95z9wfB91hdn0HHmVfwOnhneEc/Eiuf1fVpScY8C/xSRjSLysNvBNCBeVQ877/OAeDeD8cHjzmJts92+pXQ2EUkCLgLWE8Tf61lxQpB9p86tmi3AETzrJ+0FilW12jmlvkUBm9XZcarq6e/z1873+YKIRLsY4ml/AP4dqHW2u3Ke36clFfdcqqqjgRvw3Ga43O2AfKGeMehB+deW40/AADy3Gw4D/8/dcL4mIh2A94EfqWqp97Fg+l7riDPovlNVrVHVUXjWWkoFhrgcUp3OjlNEhgNP4Yl3LNAFeNLFEBGRm4AjqrrRH/VZUnGJquY6/z0C/APPD0awyj+9Xo3z3yMux1MvVc13fpBrgb8QJN+riETi+UX9pqr+3dkddN9rXXEG63cKoKrFwApgPBArnlVhoY5FAd3kFeck5zajOosK/hX3v88JwGQROQDMw3Pb60XO8/u0pOICEWkvIh1Pv8ezCFn6uUu5agEww3k/A/jQxVjOSb65WNutBMH36tyffg3YoarPex0Kqu+1vjiD7TsVkTgRiXXetwWuw9P/swK43TktGL7PuuLc6fWHhODpp3D1+1TVp1Q1QVWT8CymuFxV7+Y8v097ot4FItIfT+sEPAulvaWqv3YxpDNE5G3gSjzTXucDzwAfAO8AffAsFXCnqrreQV5PrFfiuU2jwAHg+179Fq4QkUuBlcBXfH3P+md4+iuC5ns9R5zTCaLvVERG4Ok4Dsfzh/E7qvqc83M1D88tpc3APV5LjAdTnMuBOECALcAjXh36rhKRK4F/U9Wbzvf7tKRijDHGb+z2lzHGGL+xpGKMMcZvLKkYY4zxG0sqxhhj/MaSijHGGL+xpGJMkBGRJO+Zl40JJZZUjDHG+I0lFWOCmIj0d9a4GOt2LMb4IqLhU4wxbhCRC/A80TxTVbe6HY8xvrCkYkxwisMz19Jtqrrd7WCM8ZXd/jImOJUAWcClbgdiTGNYS8WY4FSJZ0bgJSJyQlXfcjsgY3xhScWYIKWqJ50FlJY6iWWB2zEZ0xCbpdgYY4zfWJ+KMcYYv7GkYowxxm8sqRhjjPEbSyrGGGP8xpKKMcYYv7GkYowxxm8sqRhjjPGb/w96bJC+9dW5pwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJkeeph5uAK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7068abf6-6832-4a3b-93d2-a1408b4bf6c3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.metrics import (accuracy_score, precision_score, \r\n",
        "                             recall_score, f1_score, log_loss)\r\n",
        "\r\n",
        "# Step 1: Get training and testing datasets\r\n",
        "#X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.4, random_state=777)\r\n",
        "\r\n",
        "# Data normalization\r\n",
        "# normalizer = MinMaxScaler(feature_range=(0, 1))\r\n",
        "# normalizer.fit(X_tr)\r\n",
        "# X_tr_normalized = normalizer.transform(X_tr)\r\n",
        "# X_ts_normalized = normalizer.transform(X_ts)\r\n",
        "\r\n",
        "# Step 2: Use GridSearchCV to find optimal hyperparameter values\r\n",
        "clf = RandomForestClassifier()\r\n",
        "parameters = {'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450, 500],\r\n",
        "              'criterion': ['gini', 'entropy']}\r\n",
        "gridsearch = GridSearchCV(clf, parameters, scoring='accuracy', cv=3)\r\n",
        "\r\n",
        "gridsearch.fit(X_tr, Y_tr)\r\n",
        "print(f'gridsearch.best_params_ = {gridsearch.best_params_}')\r\n",
        "\r\n",
        "# Step 3: Get model with best hyperparameters\r\n",
        "best_clf = gridsearch.best_estimator_\r\n",
        "\r\n",
        "# Step 4: Get best model performance from testing set\r\n",
        "y_pred = best_clf.predict(X_ts)\r\n",
        "test_acc = accuracy_score(Y_ts, y_pred)\r\n",
        "print(f'test_acc = {test_acc}')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gridsearch.best_params_ = {'criterion': 'entropy', 'n_estimators': 300}\n",
            "test_acc = 0.5650442047993782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clO375Og16xk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}